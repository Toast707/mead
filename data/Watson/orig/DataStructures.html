<h1 id="firstHeading" class="firstHeading" lang="en">Data structure</h1>
<p>In computer science, a <b>data structure</b> is a particular way of organizing data in a computer so that it can be used efficiently.</p>
<p>Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, databases use B-tree indexes for small percentages of data retrieval and compilers and databases use dynamic hash tables as look up tables.</p>
<p>Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both main memory and in secondary memory.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Overview</h2>
<p>Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer –  a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations; while the linked data structures are based on storing addresses of data items within the structure itself. Many data structures use both principles, sometimes combined in non-trivial ways (as in XOR linking).</p>
<p>The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).</p>
<h2>Examples</h2>
<p>There are numerous types of data structures, generally built upon simpler primitive data types:</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Array data structure</h1>
<p>In computer science, an <b>array data structure</b> or simply an <b>array</b> is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored so that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.</p>
<p>For example, an array of 10 32-bit integer variables, with indices 0 through 9, may be stored as 10 words at memory addresses 2000, 2004, 2008, … 2036, so that the element with index i has the address 2000 + 4 × i.</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Associative array</h1>
<p>In computer science, an <b>associative array</b>, <b>map</b>, <b>symbol table</b>, or <b>dictionary</b> is an abstract data type composed of a collection of  pairs, such that each possible key appears at most once in the collection.</p>
<p>Operations associated with this data type allow:</p>
<p>The <b>dictionary problem</b> is a classic computer science problem: the task of designing a data structure that maintains a set of data during 'search' 'delete' and 'insert' operations. A standard solution to the dictionary problem is a hash table; in some cases it is also possible to solve the problem using directly addressed arrays, binary search trees, or other more specialized structures.</p>
<p>Many programming languages include associative arrays as primitive data types, and they are available in software libraries for many others. Content-addressable memory is a form of direct hardware-level support for associative arrays.</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Record (computer science)</h1>
<p>In computer science, a <b>record</b> (also called <b>struct</b> or <b>compound data</b>) is a basic data structure (a <b>tuple</b> may or may not be considered a record, and vice versa, depending on conventions and the language at hand). A record is a collection of elements, typically in fixed number and sequence and typically indexed by names. The elements of records may also be called fields or members.</p>
<p>For example, a date could be stored as a record containing a numeric year field, a month field represented as a string, and a numeric day-of-month field. As another example, a Personnel record might contain a name, a salary, and a rank. As yet another example, a Circle record might contain a center and a radius. In this instance, the center itself might be represented as a Point record containing x and y coordinates.</p>
<h2>Language support</h2>
<p>Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.</p>
<p>Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and Microsoft's .NET Framework.</p>
<p>Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java and Smalltalk may use classes for this purpose.</p>
<p>Many known data structures have concurrent versions that allow multiple computing threads to access the data structure simultaneously.</p>
<h2>See also</h2>
<h2>References</h2>
<h2>Further reading</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term "vector" is used in computing to refer to an array, although tuples rather than vectors are more correctly the mathematical equivalent. Arrays are often used to implement tables, especially lookup tables; the word table is sometimes used as a synonym of array.</p>
<p>Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.</p>
<p>Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.</p>
<p>The term array is often used to mean array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.</p>
<p>The term is also used, especially in the description of algorithms, to mean associative array or "abstract array", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>History</h2>
<p>The first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. Von Neumann wrote the first array-sorting program (merge sort) in 1945, during the building of the first stored-program computer.p. 159 Array indexing was originally done by self-modifying code, and later using index registers and indirect addressing. Some mainframes designed in the 1960s, such as the Burroughs B5000 and its successors, used memory segmentation to perform index-bounds checking in hardware.</p>
<p>Associative arrays have many applications including such fundamental programming patterns as memoization and the decorator pattern.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Operations</h2>
<p>In an associative array, the association between a key and a value is often known as a "binding", and the same word "binding" may also be used to refer to the process of creating a new association.</p>
<p>The operations that are usually defined for an associative array are:</p>
<p>In addition, associative arrays may also include other operations such as determining the number of bindings or constructing an iterator to loop over all the bindings. Usually, for such an operation, the order in which the bindings are returned may be arbitrary.</p>
<p>A multimap generalizes an associative array by allowing multiple values to be associated with a single key. A bidirectional map is a related abstract data type in which the bindings operate in both directions: each value must be associated with a unique key, and a second lookup operation takes a value as argument and looks up the key associated with that value.</p>
<h2>Example</h2>
<p>Records are distinguished from arrays by the fact that their number of fields is typically fixed, each field has a name, and that each field may have a different type.</p>
<p>A <b>record type</b> is a data type that describes such values and variables. Most modern computer languages allow the programmer to define new record types. The definition includes specifying the data type of each field and an identifier (name or label) by which it can be accessed. In type theory, product types (with no field names) are generally preferred due to their simplicity, but proper record types are studied in languages such as System F-sub. Since type-theoretical records may contain first-class function-typed fields in addition to data, they can express many features of object-oriented programming.</p>
<p>Records can exist in any storage medium, including main memory and mass storage devices such as magnetic tapes or hard disks. Records are a fundamental component of most data structures, especially linked data structures. Many computer files are organized as arrays of logical records, often grouped into larger physical records or blocks for efficiency.</p>
<p>The parameters of a function or procedure can often be viewed as the fields of a record variable; and the arguments passed to that function can be viewed as a record value that gets assigned to that variable at the time of the call. Also, in the call stack that is often used to implement procedure calls, each entry is an activation record or call frame, containing the procedure parameters and local variables, the return address, and other internal fields.</p>
<p>An object in object-oriented language is essentially a record that contains procedures specialized to handle that record; and object types are an elaboration of record types. Indeed, in most object-oriented languages, records are just special cases of objects, and are known as plain old data structures (PODSs), to contrast with objects that use OO features.</p>
<p>A record can be viewed as the computer analog of a mathematical tuple. In the same vein, a record type can be viewed as the computer language analog of the Cartesian product of two or more mathematical sets, or the implementation of an abstract product type in a specific language.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>History</h2>
<p>The concept of record can be traced to various types of tables and ledgers used in accounting since remote times. The modern notion of records in computer science, with fields of well-defined type and size, was already implicit in 19th century mechanical calculators, such as Babbage's Analytical Engine.</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Binary heap</h1>
<p>A <b>binary heap</b> is a heap data structure created using a binary tree. It can be seen as a binary tree with two additional constraints:</p>
<p>Heaps with a mathematical "greater than or equal to" (≥) comparison predicate are called max-heaps; those with a mathematical "less than or equal to" (≤) comparison predicate are called min-heaps. Min-heaps are often used to implement priority queues.</p>
<p>Since the ordering of siblings in a heap is not specified by the heap property, a single node's two children can be freely interchanged unless doing so violates the shape property (compare with treap).</p>
<p>The binary heap is a special case of the d-ary heap in which d = 2.</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Heap (data structure)</h1>
<p>In computer science, a <b>heap</b> is a specialized tree-based data structure that satisfies the heap property: If A is a parent node of B then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap. Either the keys of parent nodes are always greater than or equal to those of the children and the highest key is in the root node (this kind of heap is called max heap) or the keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node (min heap). Heaps are crucial in several efficient graph algorithms such as Dijkstra's algorithm, and in the sorting algorithm heapsort. A common implementation of a heap is the binary heap, in which the tree is a complete binary tree (see figure).</p>
<p>Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including FORTRAN (1957), COBOL (1960), and ALGOL 60 (1960), had support for multi-dimensional arrays, and so has C (1972). In C++ (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime as well as for runtime-flexible arrays.</p>
<h2>Applications</h2>
<p>Arrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.</p>
<p>Arrays are used to implement other data structures, such as heaps, hash tables, deques, queues, stacks, strings, and VLists.</p>
<p>One or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate "dynamic memory" portably.</p>
<p>Arrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive) multiple <strong>IF</strong> statements. They are known in this context as control tables and are used in conjunction with a purpose built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) that direct the path of the execution.</p>
<h2>Element identifier and addressing formulas</h2>
<p>When data objects are stored in an array, individual objects are selected by an index that is usually a non-negative scalar integer. Indices are also called subscripts. An index maps the array value to a stored object.</p>
<p>There are three ways in which the elements of an array can be indexed:</p>
<p>Arrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example a two-dimensional array <strong>A</strong> with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression <strong>A</strong> (in a row major language) or <strong>A</strong> (in a column major language) in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and n for an n-dimensional array.</p>
<p>The number of indices needed to specify an element is called the dimension, dimensionality, or rank of the array.</p>
<p>In standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some enumerated type), and the address of an element is computed by a "linear" formula on the indices.</p>
<p>Suppose that the set of loans made by a library is to be represented in a data structure. Each book in a library may be checked out only by a single library patron at a time. However, a single patron may be able to check out multiple books. Therefore, the information about which books are checked out to which patrons may be represented by an associative array, in which the books are the keys and the patrons are the values. For instance (using notation from Python, or JSON (JavaScript Object Notation), in which a binding is represented by placing a colon between the key and the value), the current checkouts may be represented by an associative array</p>
<p><i>
{
    "Great Expectations": "John",
    "Pride and Prejudice": "Alice",
    "Wuthering Heights": "Alice"
}
</i></p>
<p>A lookup operation with the key "Great Expectations" in this array would return the name of the person who checked out that book, John. If John returns his book, that would cause a deletion operation in the associative array, and if Pat checks out another book, that would cause an insertion operation, leading to a different state:</p>
<p><i>
{
    "Pride and Prejudice": "Alice",
    "The Brothers Karamazov": "Pat",
    "Wuthering Heights": "Alice"
}
</i></p>
<p>In this new state, the same lookup as before, with the key "Great Expectations", would raise an exception, because this key is no longer present in the array.</p>
<h2>Implementation</h2>
<p>For dictionaries with very small numbers of bindings, it may make sense to implement the dictionary using an association list, a linked list of bindings. With this implementation, the time to perform the basic dictionary operations is linear in the total number of bindings; however, it is easy to implement and the constant factors in its running time are small.</p>
<p>Another very simple implementation technique, usable when the keys are restricted to a narrow range of integers, is direct addressing into an array: the value for a given key k is stored at the array cell A, or if there is no binding for k then the cell stores a special sentinel value that indicates the absence of a binding. As well as being simple, this technique is fast: each dictionary operation takes constant time. However, the space requirement for this structure is the size of the entire keyspace, making it impractical unless the keyspace is small.</p>
<p>The most frequently used general purpose implementation of an associative array is with a hash table: an array of bindings, together with a hash function that maps each possible key into an array index. The basic idea of a hash table is that the binding for a given key is stored at the position given by applying the hash function to that key, and that lookup operations are performed by looking at that cell of the array and using the binding found there. However, hash table based dictionaries must be prepared to handle collisions that occur when two keys are mapped by the hash function to the same index, and many different collision resolution strategies have been developed for dealing with this situation, often based either on open addressing (looking at a sequence of hash table indices instead of a single index, until finding either the given key or an empty cell) or on hash chaining (storing a small association list instead of a single binding in each hash table cell).</p>
<p>Dictionaries may also be stored in binary search trees or in data structures specialized to a particular type of keys such as radix trees, tries, Judy arrays, or van Emde Boas trees, but these implementation methods are less efficient than hash tables as well as placing greater restrictions on the types of data that they can handle. The advantages of these alternative structures come from their ability to handle operations beyond the basic ones of an associative array, such as finding the binding whose key is the closest to a queried key, when the query is not itself present in the set of bindings.</p>
<h2>Language support</h2>
<p>Records were well established in the first half of the 20th century, when most data processing was done using punched cards. Typically each record of a data file would be recorded in one punched card, with specific columns assigned to specific fields. Generally, a record was the smallest unit that could be read in from external storage (e.g. card reader, tape or disk).</p>
<p>Most machine language implementations and early assembly languages did not have special syntax for records, but the concept was available (and extensively used) through the use of index registers, indirect addressing, and self-modifying code. Some early computers, such as the IBM 1620, had hardware support for delimiting records and fields, and special instructions for copying such records.</p>
<p>The concept of records and fields was central in some early file sorting and tabulating utilities, such as IBM's Report Program Generator (RPG).</p>
<p>COBOL was the first widespread programming language to support record types, and its record definition facilities were quite sophisticated at the time. The language allows for the definition of nested records with alphanumeric, integer, and fractional fields of arbitrary size and precision, as well as fields that automatically format any value assigned to them (e.g., insertion of currency signs, decimal points, and digit group separators). Each file is associated with a record variable where data is read into or written from. COBOL also provides a <strong>MOVE</strong> <strong>CORRESPONDING</strong> statement that assigns corresponding fields of two records according to their names.</p>
<p>The early languages developed for numeric computing, such as FORTRAN (up to FORTRAN IV) and Algol 60, did not have support for record types; but latter versions of those languages, such as Fortran 77 and Algol 68 did add them. The original Lisp programming language too was lacking records (except for the built-in cons cell), but its S-expressions provided an adequate surrogate. The Pascal programming language was one of the first languages to fully integrate record types with other basic types into a logically consistent type system. IBM's PL/1 programming language provided for COBOL-style records. The C programming language initially provided the record concept as a kind of template (<strong>struct</strong>) that could be laid on top of a memory area, rather than a true record data type. The latter were provided eventually (by the <strong>typedef</strong> declaration), but the two concepts are still distinct in the language. Most languages designed after Pascal (such as Ada, Modula, and Java) also supported records.</p>
<h2>Operations</h2>
<p>A programming language that supports record types usually provides some or all of the following operations:</p>
<p>The selection of a field from a record value yields a value.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Heap operations</h2>
<p>Both the insert and remove operations modify the heap to conform to the shape property first, by adding or removing from the end of the heap. Then the heap property is restored by traversing up or down the heap. Both operations take O(log n) time.</p>
<p>To add an element to a heap we must perform an up-heap operation (also known as bubble-up, percolate-up, sift-up, trickle-up, heapify-up, or cascade-up), by following this algorithm:</p>
<p>The number of operations required is dependent on the number of levels the new element must rise to satisfy the heap property, thus the insertion operation has a time complexity of O(log n). However, in 1974, Thomas Porter and Istvan Simon proved that the function for the average number of levels an inserted node moves up is upper bounded by the constant 1.6067. The average number of operations required for an insertion into a binary heap is 2.6067 since one additional comparison is made that does not result in the inserted node moving up a level. Thus, on average, binary heap insertion has a constant, O(1), time complexity. Intuitively, this makes sense since approximately 50% of the elements are leaves and approximately 75% of the elements are in the bottom two levels, it is likely that the new element to be inserted will only move a few levels upwards to maintain the heap.</p>
<p>As an example of binary heap insertion, say we have a max-heap</p>
<p>and we want to add the number 15 to the heap. We first place the 15 in the position marked by the X. However, the heap property is violated since 15 is greater than 8, so we need to swap the 15 and the 8. So, we have the heap looking as follows after the first swap:</p>
<p>However the heap property is still violated since 15 is greater than 11, so we need to swap again:</p>
<p>which is a valid max-heap. There is no need to check the children after this. Before we placed 15 on X, the heap was valid, meaning 11 is greater than 5. If 15 is greater than 11, and 11 is greater than 5, then 15 must be greater than 5, because of the transitive relation.</p>
<p>The procedure for deleting the root from the heap (effectively extracting the maximum element in a max-heap or the minimum element in a min-heap) and restoring the properties is called down-heap (also known as bubble-down, percolate-down, sift-down, trickle down, heapify-down, cascade-down and extract-min/max).</p>
<p>In a heap the highest (or lowest) priority element is always stored at the root, hence the name <b>heap</b>. A heap is not a sorted structure and can be regarded as partially ordered. As visible from the Heap-diagram, there is no particular relationship among nodes on any given level, even among the siblings. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes always has O(log N) height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.</p>
<p>Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap, but in many types it is at most two, which is known as a binary heap.</p>
<p>The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact priority queues are often referred to as "heaps", regardless of how they may be implemented. Note that despite the similarity of the name "heap" to "stack" and "queue", the latter two are abstract data types, while a heap is a specific data structure, and "priority queue" is the proper term for the abstract data type.</p>
<p>A heap data structure should not be confused with the heap which is a common name for the pool of memory from which dynamically allocated memory is allocated. The term was originally used only for the data structure.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Implementation and operations</h2>
<p>Heaps are usually implemented in an array, and do not require pointers between elements.</p>
<p>Full and almost full binary heaps may be represented in a very space-efficient way using an array alone. The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position n would be at positions 2n and 2n+1 in a one-based array, or 2n+1 and 2n+2 in a zero-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by swapping elements which are out of order. As we can build a heap from an array without requiring extra memory (for the nodes, for example), heapsort can be used to sort an array in-place.</p>
<p>The operations commonly performed with a heap are:</p>
<h1 id="firstHeading" class="firstHeading" lang="en">B-tree</h1>
<h1 id="firstHeading" class="firstHeading" lang="en">Hash table</h1>
<h1 id="firstHeading" class="firstHeading" lang="en">Doubly linked list</h1>
<p>In computer science, a <b>doubly-linked list</b> is a linked data structure that consists of a set of sequentially linked records called nodes. Each node contains two fields, called links, that are references to the previous and to the next node in the sequence of nodes. The beginning and ending nodes' <b>previous</b> and <b>next</b> links, respectively, point to some kind of terminator, typically a sentinel node or null, to facilitate traversal of the list. If there is only one sentinel node, then the list is circularly linked via the sentinel node. It can be conceptualized as two singly linked lists formed from the same data items, but in opposite sequential orders.</p>
<p>The two node links allow traversal of the list in either direction. While adding or removing a node in a doubly-linked list requires changing more links than the same operations on a singly linked list, the operations are simpler and potentially more efficient (for nodes other than first nodes) because there is no need to keep track of the previous node during traversal or no need to traverse the list to find the previous node, so that its link can be modified.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Nomenclature and implementation</h2>
<p>The first and last nodes of a doubly-linked list are immediately accessible (i.e., accessible without traversal, and usually called head and tail) and therefore allow traversal of the list from the beginning or end of the list, respectively: e.g., traversing the list from beginning to end, or from end to beginning, in a search of the list for a node with specific data value. Any node of a doubly-linked list, once obtained, can be used to begin a new traversal of the list, in either direction (towards beginning or end), from the given node.</p>
<p>The link fields of a doubly-linked list node are often called <b>next</b> and <b>previous</b> or <b>forward</b> and <b>backward</b>. The references stored in the link fields are usually implemented as pointers, but (as in any linked data structure) they may also be address offsets or indices into an array where the nodes live.</p>
<h2>Basic algorithms</h2>
<p>A one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.</p>
<p>As an example consider the C declaration <strong>int anArrayName;</strong></p>
<p>Syntax : datatype anArrayname;</p>
<p>In the given example the array can contain 10 elements of any value available to the <strong>int</strong> type. In C, the array element indices are 0-9 inclusive in this case. For example, the expressions <strong>anArrayName</strong> and <strong>anArrayName</strong> are the first and last elements respectively.</p>
<p>For a vector with linear addressing, the element with index i is located at the address B + c × i, where B is a fixed base address and c a fixed constant, sometimes called the address increment or stride.</p>
<p>If the valid element indices begin at 0, the constant B is simply the address of the first element of the array. For this reason, the C programming language specifies that array indices always begin at 0; and many programmers will call that element "zeroth" rather than "first".</p>
<p>However, one can choose the index of the first element by an appropriate choice of the base address B. For example, if the array has five elements, indexed 1 through 5, and the base address B is replaced by B + 30c, then the indices of those same elements will be 31 to 35. If the numbering does not start at 0, the constant B may not be the address of any element.</p>
<p>For a two-dimensional array, the element with indices i,j would have address B + c · i + d · j, where the coefficients c and d are the row and column address increments, respectively.</p>
<p>More generally, in a k-dimensional array, the address of an element with indices i<sub>1</sub>, i<sub>2</sub>, …, i<sub>k</sub> is</p>
<p>For example: int a;</p>
<p>This means that array a has 3 rows and 2 columns, and the array is of integer type. Here we can store 6 elements they are stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a<sub>11</sub>, a<sub>12</sub>, a<sub>13</sub>, a<sub>21</sub>, a<sub>22</sub>, a<sub>23</sub>.</p>
<p>This formula requires only k multiplications and k additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by bit shifting.</p>
<p>Associative arrays can be implemented in any programming language as a package and many language systems provide them as part of their standard library. In some languages, they are not only built into the standard system, but have special syntax, often using array-like subscripting.</p>
<p>Built-in syntactic support for associative arrays was introduced by SNOBOL4, under the name "table". MUMPS made multi-dimensional associative arrays, optionally persistent, its key data structure. SETL supported them as one possible implementation of sets and maps. Most modern scripting languages, starting with AWK and including Rexx, Perl, Tcl, JavaScript, Python, Ruby, and Lua, support associative arrays as a primary container type. In many more languages, they are available as library functions without special syntax.</p>
<p>In Smalltalk, Objective-C, .NET, Python, REALbasic, and Swift they are called dictionaries; in Perl, Ruby and Seed7 they are called hashes; in C++, Java, Go, Clojure, Scala, OCaml, Haskell they are called maps (see map (C++), unordered_map (C++), and <strong>Map</strong>); in Common Lisp and Windows PowerShell, they are called hash tables (since both typically use this implementation). In PHP, all arrays can be associative, except that the keys are limited to integers and strings. In JavaScript (see also JSON), all objects behave as associative arrays. In Lua, they are called tables, and are used as the primitive building block for all data structures. In Visual FoxPro, they are called Collections. The D language also has support for associative arrays </p>
<h2>See also</h2>
<h2>References</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>Some languages may provide facilities that enumerate all fields of a record, or at least the fields that are references. This facility is needed to implement certain services such as debuggers, garbage collectors, and serialization. It requires some degree of type polymorphism.</p>
<p>In systems with record subtyping, operations on values of record type may also include:</p>
<p>In such settings, a specific record type implies that a specific set of fields are present, but values of that type may contain additional fields. A record with fields x, y, and z would thus belong to the type of records with fields x and y, as would a record with fields x, y, and r. The rationale is that passing an (x,y,z) record to a function that expects an (x,y) record as argument should work, since that function will find all the fields it requires within the record. Many ways of practically implementing records in programming languages would have trouble with allowing such variability, but the matter is a central characteristic of record types in more theoretical contexts.</p>
<p>Most languages allow assignment between records that have exactly the same record type (including same field types and names, in the same order). Depending on the language, however, two record data types defined separately may be regarded as distinct types even if they have exactly the same fields.</p>
<p>Some languages may also allow assignment between records whose fields have different names, matching each field value with the corresponding field variable by their positions within the record; so that, for example, a complex number with fields called <strong>real</strong> and <strong>imag</strong> can be assigned to a 2D point record variable with fields <strong>X</strong> and <strong>Y</strong>. In this alternative, the two operands are still required to have the same sequence of field types. Some languages may also require that corresponding types have the same size and encoding as well, so that the whole record can be assigned as an uninterpreted bit string. Other languages may be more flexible in this regard, and require only that each value field can be legally assigned to the corresponding variable field; so that, for example, a short integer field can be assigned to a long integer field, or vice-versa.</p>
<p>Other languages (such as COBOL) may match fields and values by their names, rather than positions.</p>
<p>These same possibilities apply to the comparison of two record values for equality. Some languages may also allow order comparisons ('&lt;'and '&gt;'), using the lexicographic order based on the comparison of individual fields.</p>
<p>PL/I allows both of the preceding types of assignment, and also allows structure expressions, such as <strong>a = a+1;</strong> where "a" is a record, or structure in PL/I terminology.</p>
<p>In Algol 68, if <strong>Pts</strong> was an array of records, each with integer fields <strong>X</strong> and <strong>Y</strong>, one could write <strong>Pts.Y</strong> to obtain an array of integers, consisting of the <strong>Y</strong> fields of all the elements of <strong>Pts</strong>. As a result, the statements <strong>Pts.Y := 7</strong> and <strong>Pts.Y := 7</strong> would have the same effect.</p>
<p>In the Pascal programming language, the command <strong>with R do S</strong> would execute the command sequence <strong>S</strong> as if all the fields of record <strong>R</strong> had been declared as variables. So, instead of writing <strong>Pt.X := 5; Pt.Y := Pt.X + 3</strong> one could write <strong>with Pt do begin X := 5; Y := X + 3 end</strong>.</p>
<h2>Representation in memory</h2>
<p>The representation of records in memory varies depending on the programming languages. Usually the fields are stored in consecutive positions in memory, in the same order as they are declared in the record type. This may result in two or more fields stored into the same word of memory; indeed, this feature is often used in systems programming to access specific bits of a word. On the other hand, most compilers will add padding fields, mostly invisible to the programmer, in order to comply with alignment constraints imposed by the machine—say, that a floating point field must occupy a single word.</p>
<p>So, if we have the same max-heap as before</p>
<p>We remove the 11 and replace it with the 4.</p>
<p>Now the heap property is violated since 8 is greater than 4. In this case, swapping the two elements, 4 and 8, is enough to restore the heap property and we need not swap elements further:</p>
<p>The downward-moving node is swapped with the larger of its children in a max-heap (in a min-heap it would be swapped with its smaller child), until it satisfies the heap property in its new position. This functionality is achieved by the <b>Max-Heapify</b> function as defined below in pseudocode for an array-backed heap A of length heap_length. Note that "A" is indexed starting at 1, not 0 as is common in many real programming languages.</p>
<p><b>Max-Heapify</b> (A, i):<br>
 left ← 2i<br>
 right ← 2i + 1<br>
 largest ← i<br>
 <b>if</b> left ≤ heap_length <b>and</b> A &gt; A <b>then</b>:<br>
 largest ← left<br>
 <b>if</b> right ≤ heap_length <b>and</b> A &gt; A <b>then</b>:<br>
 largest ← right<br>
 <b>if</b> largest ≠ i <b>then</b>:<br>
 <b>swap</b> A ↔ A<br>  Max-Heapify(A, largest)</p>
<p>Different types of heaps implement the operations in different ways, but notably, insertion is often done by adding the new element at the end of the heap in the first available free space. This will tend to violate the heap property, and so the elements are then reordered until the heap property has been reestablished. Construction of a binary (or d-ary) heap out of a given array of elements may be performed faster than a sequence of consecutive insertions into an originally empty heap using the classic Floyd algorithm, with the worst-case number of comparisons equal to 2N − 2s<sub>2</sub>(N) − e<sub>2</sub>(N) (for a binary heap), where s<sub>2</sub>(N) is the sum of all digits of the binary representation of N and e<sub>2</sub>(N) is the exponent of 2 in the prime factorization of N.</p>
<h2>Variants</h2>
<h2>Comparison of theoretic bounds for variants</h2>
<p>In the following time complexities O(f) is an asymptotic upper bound and Θ(f) is an asymptotically tight bound (see Big O notation). Function names assume a min-heap.</p>
<h2>Applications</h2>
<p>The heap data structure has many applications.</p>
<h2>Implementations</h2>
<h2>See also</h2>
<h2>References</h2>
<p>In computer science, a <b>B-tree</b> is a tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a binary search tree in that a node can have more than two children (Comer 1979, p. 123). Unlike self-balancing binary search trees, the B-tree is optimized for systems that read and write large blocks of data. It is commonly used in databases and filesystems.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Overview</h2>
<p>In B-trees, internal (non-leaf) nodes can have a variable number of child nodes within some pre-defined range. When data is inserted or removed from a node, its number of child nodes changes. In order to maintain the pre-defined range, internal nodes may be joined or split. Because a range of child nodes is permitted, B-trees do not need re-balancing as frequently as other self-balancing search trees, but may waste some space, since nodes are not entirely full. The lower and upper bounds on the number of child nodes are typically fixed for a particular implementation. For example, in a 2-3 B-tree (often simply referred to as a <b>2-3 tree</b>), each internal node may have only 2 or 3 child nodes.</p>
<p>Each internal node of a B-tree will contain a number of keys. The keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: a<sub>1</sub> and a<sub>2</sub>. All values in the leftmost subtree will be less than a<sub>1</sub>, all values in the middle subtree will be between a<sub>1</sub> and a<sub>2</sub>, and all values in the rightmost subtree will be greater than a<sub>2</sub>.</p>
<p>Usually, the number of keys is chosen to vary between  and , where  is the minimum number of keys, and  is the minimum degree or branching factor of the tree. In practice, the keys take up the most space in a node. The factor of 2 will guarantee that nodes can be split or combined. If an internal node has  keys, then adding a key to that node can be accomplished by splitting the  key node into two  key nodes and adding the key to the parent node. Each split node has the required minimum number of keys. Similarly, if an internal node and its neighbor each have  keys, then a key may be deleted from the internal node by combining with its neighbor. Deleting the key would make the internal node have  keys; joining the neighbor would add  keys plus one more key brought down from the neighbor's parent. The result is an entirely full node of  keys.</p>
<p>The number of branches (or child nodes) from a node will be one more than the number of keys stored in the node. In a 2-3 B-tree, the internal nodes will store either one key (with two child nodes) or two keys (with three child nodes). A B-tree is sometimes described with the parameters  —  or simply with the highest branching order, .</p>
<p>A B-tree is kept balanced by requiring that all leaf nodes be at the same depth. This depth will increase slowly as elements are added to the tree, but an increase in the overall depth is infrequent, and results in all leaf nodes being one more node farther away from the root.</p>
<p>In computing, a <b>hash table</b> (<b>hash map</b>) is a data structure used to implement an associative array, a structure that can map keys to values. A hash table uses a hash function to compute an index into an array of buckets or slots, from which the correct value can be found.</p>
<p>Ideally, the hash function will assign each key to a unique bucket, but this situation is rarely achievable in practice (usually some keys will hash to the same bucket). Instead, most hash table designs assume that hash collisions—different keys that are assigned by the hash function to the same bucket—will occur and must be accommodated in some way.</p>
<p>In a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized) constant average cost per operation.</p>
<p>In many situations, hash tables turn out to be more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Hashing</h2>
<p>The idea of hashing is to distribute the entries (key/value pairs) across an array of buckets. Given a key, the algorithm computes an index that suggests where the entry can be found:</p>
<p><i>
index = f(key, array_size)
</i></p>
<p>Often this is done in two steps:</p>
<p><i>
<b>record</b> DoublyLinkedNode {
    prev // A reference to the previous node
    next // A reference to the next node
    data // Data or a reference to data
 }
</i></p>
<p><i>
<b>record</b> DoublyLinkedList {
     DoublyLinkedNode firstNode   // points to first node of list
     DoublyLinkedNode lastNode    // points to last node of list
}
</i></p>
<p>Traversal of a doubly-linked list can be in either direction. In fact, the direction of traversal can change many times, if desired. <b>Traversal</b> is often called <b>iteration</b>, but that choice of terminology is unfortunate, for <b>iteration</b> has well-defined semantics (e.g., in mathematics) which are not analogous to <b>traversal</b>.</p>
<p>Forwards</p>
<p><i>
node  := list.firstNode
 <b>while</b> node ≠ <b>null</b>
     &lt;do something with node.data&gt;
     node  := node.next
</i></p>
<p>Backwards</p>
<p><i>
node  := list.lastNode
 <b>while</b> node ≠ <b>null</b>
     &lt;do something with node.data&gt;
     node  := node.prev
</i></p>
<h1 id="firstHeading" class="firstHeading" lang="en">Stack (abstract data type)</h1>
<h1 id="firstHeading" class="firstHeading" lang="en">Binary search tree</h1>
<p>In computer science, a <b>binary search tree</b> (<b>BST</b>), sometimes also called an <b>ordered</b> or <b>sorted binary tree</b>, is a node-based binary tree data structure where each node has a comparable key (and an associated value) and satisfies the restriction that the key in any node is larger than the keys in all nodes in that node's left sub-tree and smaller than the keys in all nodes in that node's right sub-tree. Each node has no more than two child nodes. Each child must either be a leaf node or the root of another binary search tree. The left sub-tree contains only nodes with keys less than the parent node; the right sub-tree contains only nodes with keys greater than the parent node. BSTs are also dynamic data structures, and the size of a BST is only limited by the amount of free memory in the operating system. The main advantage of binary search trees is that it remains ordered, which provides quicker search times than many other data structures. The common properties of binary search trees are as follows:</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Quicksort</h1>
<h1 id="firstHeading" class="firstHeading" lang="en">Heapsort</h1>
<p><b>Heapsort</b> is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the smallest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the minimum.</p>
<p>Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort.</p>
<p>The coefficients c<sub>k</sub> must be chosen so that every valid index tuple maps to the address of a distinct element.</p>
<p>If the minimum legal value for every index is 0, then B is the address of the element whose indices are all zero. As in the one-dimensional case, the element indices may be changed by changing the base address B. Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing B by B + c<sub>1</sub> - − 3 c<sub>1</sub> will cause them to be renumbered from 0 through 9 and 4 through 23, respectively. Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition; while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.</p>
<p>The addressing formula is completely defined by the dimension d, the base address B, and the increments c<sub>1</sub>, c<sub>2</sub>, …, c<sub>k</sub>. It is often useful to pack these parameters into a record called the array's descriptor or stride vector or dope vector. The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector. The dope vector is a complete handle for the array, and is a convenient way to pass arrays as arguments to procedures. Many useful array slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.</p>
<p>Often the coefficients are chosen so that the elements occupy a contiguous area of memory. However, that is not necessary. Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.</p>
<p>There are two systematic compact layouts for a two-dimensional array. For example, consider the matrix</p>
<p>In the row-major order layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:</p>
<p>In column-major order (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:</p>
<p>For arrays with three or more indices, "row major order" puts in consecutive positions any two elements whose index tuples differ only by one in the last index. "Column major order" is analogous with respect to the first index.</p>
<p>In systems which use processor cache or virtual memory, scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered. Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array. For example, when computing the product A·B of two matrices, it would be best to have A stored in row-major order, and B in column-major order.</p>
<p>Static arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a dynamic version of an array; see dynamic array. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.</p>
<p>Some languages may implement a record as an array of addresses pointing to the fields (and, possibly, to their names and/or types). Objects in object-oriented languages are often implemented in rather complicated ways, especially in languages that allow multiple class inheritance.</p>
<h2>Examples</h2>
<p>The following show examples of record definitions:</p>
<p><i>
  declare 1 date,
            2 year  picture '9999',
            2 month picture '99',
            2 day   picture '99';
</i></p>
<p><i>
struct date {
   int year;
   int month;
   int day;
};
</i></p>
<h2>See also</h2>
<h2>References</h2>
<h2>Navigation menu</h2>
<p>For the above algorithm to correctly re-heapify the array, the node at index i and its two direct children must violate the heap property. If they do not, the algorithm will fall through with no change to the array. The down-heap operation (without the preceding swap) can also be used to modify the value of the root, even when an element is not being deleted.</p>
<p>In the worst case, the new root has to be swapped with its child on each level until it reaches the bottom level of the heap, meaning that the delete operation has a time complexity relative to the height of the tree, or O(log n).</p>
<h2>Building a heap</h2>
<p>A heap could be built by successive insertions. This approach requires  time because each insertion takes  time and there are  elements. However this is not the optimal method. The optimal method starts by arbitrarily putting the elements on a binary tree, respecting the shape property (the tree could be represented by an array, see below). Then starting from the lowest level and moving upwards, shift the root of each subtree downward as in the deletion algorithm until the heap property is restored. More specifically if all the subtrees starting at some height  (measured from the bottom) have already been "heapified", the trees at height  can be heapified by sending their root down along the path of maximum valued children when building a max-heap, or minimum valued children when building a min-heap. This process takes  operations (swaps) per node. In this method most of the heapification takes place in the lower levels. Since the height of the heap is , the number of nodes at height  is . Therefore, the cost of heapifying all subtrees is:</p>
<p>This uses the fact that the given infinite series h / 2h converges to 2.</p>
<p>The exact value of the above (the worst-case number of comparisons during the heap construction) is known to be equal to:</p>
<p>where s<sub>2</sub>(n) is the sum of all digits of the binary representation of n and e<sub>2</sub>(n) is the exponent of 2 in the prime factorization of n.</p>
<p>The <b>Build-Max-Heap</b> function that follows, converts an array A which stores a complete binary tree with n nodes to a max-heap by repeatedly using <b>Max-Heapify</b> in a bottom up manner. It is based on the observation that the array elements indexed by floor(n/2) + 1, floor(n/2) + 2, ..., n are all leaves for the tree, thus each is a one-element heap. <b>Build-Max-Heap</b> runs <b>Max-Heapify</b> on each of the remaining tree nodes.</p>
<p><b>Build-Max-Heap</b> (A):<br>
 heap_length ← length<br>
 <b>for</b> i ← floor(length/2) <b>downto</b> 1 <b>do</b><br>
 <b>Max-Heapify</b>(A, i)</p>
<h2>Heap implementation</h2>
<p>Heaps are commonly implemented with an array. Any binary tree can be stored in an array, but because a binary heap is always a complete binary tree, it can be stored compactly. No space is required for pointers; instead, the parent and children of each node can be found by arithmetic on array indices. These properties make this heap implementation a simple example of an implicit data structure or Ahnentafel list. Details depend on the root position, which in turn may depend on constraints of a programming language used for implementation, or programmer preference. Specifically, sometimes the root is placed at index 1, sacrificing space in order to simplify arithmetic. The peek operation (find-min or find-max) simply returns the value of the root, and is thus O(1).</p>
<p>Let n be the number of elements in the heap and i be an arbitrary valid index of the array storing the heap. If the tree root is at index 0, with valid indices 0 through n − 1, then each element a at index i has</p>
<p>Alternatively, if the tree root is at index 1, with valid indices 1 through n, then each element a at index i has</p>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>B-trees have substantial advantages over alternative implementations when otherwise the time to access the data of a node greatly exceeds the time spent processing that data, because then the cost of accessing the node may be amortized over multiple operations within the node. This usually occurs when the node data are in secondary storage such as disk drives. By maximizing the number of keys within each internal node, the height of the tree decreases and the number of expensive node accesses is reduced. In addition, rebalancing of the tree occurs less often. The maximum number of child nodes depends on the information that must be stored for each child node and the size of a full disk block or an analogous size in secondary storage. While 2-3 B-trees are easier to explain, practical B-trees using secondary storage need a large number of child nodes to improve performance.</p>
<p><i>
hash = hashfunc(key)
index = hash % array_size
</i></p>
<p>In this method, the hash is independent of the array size, and it is then reduced to an index (a number between <strong>0</strong> and <strong>array_size − 1</strong>) using the modulo operator (<strong>%</strong>).</p>
<p>In the case that the array size is a power of two, the remainder operation is reduced to masking, which improves speed, but can increase problems with a poor hash function.</p>
<p>A good hash function and implementation algorithm are essential for good hash table performance, but may be difficult to achieve.</p>
<p>A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g. a Pearson's chi-squared test for discrete uniform distributions.</p>
<p>The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size s, then the hash function needs to be uniform only when s is a power of two. On the other hand, some hashing algorithms provide uniform hashes only when s is a prime number.</p>
<p>For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash is claimed to have particularly poor clustering behavior.</p>
<p>Cryptographic hash functions are believed to provide good hash functions for any table size s, either by modulo reduction or by bit masking. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function).</p>
<p>These symmetric functions insert a node either after or before a given node:</p>
<p><i>
<b>function</b> insertAfter(List list, Node node, Node newNode)
     newNode.prev  := node
     newNode.next  := node.next
     <b>if</b> node.next == <b>null</b>
         list.lastNode  := newNode
     <b>else</b>
         node.next.prev  := newNode
     node.next  := newNode
</i></p>
<p><i>
<b>function</b> insertBefore(List list, Node node, Node newNode)
     newNode.prev  := node.prev
     newNode.next  := node
     <b>if</b> node.prev == <b>null</b>
         list.firstNode  := newNode
     <b>else</b>
         node.prev.next  := newNode
     node.prev  := newNode
</i></p>
<p>We also need a function to insert a node at the beginning of a possibly empty list:</p>
<p><i>
<b>function</b> insertBeginning(List list, Node newNode)
     <b>if</b> list.firstNode == <b>null</b>
         list.firstNode  := newNode
         list.lastNode   := newNode
         newNode.prev  := null
         newNode.next  := null
     <b>else</b>
         insertBefore(list, list.firstNode, newNode)
</i></p>
<p>A symmetric function inserts at the end:</p>
<p><i>
<b>function</b> insertEnd(List list, Node newNode)
     <b>if</b> list.lastNode == <b>null</b>
         insertBeginning(list, newNode)
     <b>else</b>
         insertAfter(list, list.lastNode, newNode)
</i></p>
<p>Removal of a node is easier than insertion, but requires special handling if the node to be removed is the firstNode or lastNode:</p>
<p><i>
<b>function</b> remove(List list, Node node)
   <b>if</b> node.prev == <b>null</b>
       list.firstNode  := node.next
   <b>else</b>
       node.prev.next  := node.next
   <b>if</b> node.next == <b>null</b>
       list.lastNode  := node.prev
   <b>else</b>
       node.next.prev  := node.prev
   <b>destroy</b> node
</i></p>
<p>One subtle consequence of the above procedure is that deleting the last node of a list sets both firstNode and lastNode to null, and so it handles removing the last node from a one-element list correctly. Notice that we also don't need separate "removeBefore" or "removeAfter" methods, because in a doubly-linked list we can just use "remove(node.prev)" or "remove(node.next)" where these are valid. This also assumes that the node being removed is guaranteed to exist. If the node does not exist in this list, then some error handling would be required.</p>
<p>Assuming that someNode is some node in a non-empty list, this code traverses through that list starting with someNode (any node will do):</p>
<p>In computer science, a <b>stack</b> is a particular kind of abstract data type or collection in which the principal (or only) operations on the collection are the addition of an entity to the collection, known as push and removal of an entity, known as pop. The relation between the push and pop operations is such that the stack is a Last-In-First-Out (LIFO) data structure. In a LIFO data structure, the last element added to the structure must be the first one to be removed. This is equivalent to the requirement that, considered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. Often a peek or top operation is also implemented, returning the value of the top element without removing it.</p>
<p>A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A pop either reveals previously concealed items or results in an empty stack, but, if the stack is empty, it goes into underflow state, which means no items are present in stack to be removed.</p>
<p>A stack is a restricted data structure, because only a small number of operations are performed on it. The nature of the pop and push operations also means that stack elements have a natural order. Elements are removed from the stack in the reverse order to the order of their addition. Therefore, the lower elements are those that have been on the stack the longest.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>History</h2>
<p>The stack was first proposed in 1946, in the computer design of Alan M. Turing (who used the terms "bury" and "unbury") as a means of calling and returning from subroutines. Subroutines had already been implemented in Konrad Zuse's Z4 in 1945. Klaus Samelson and Friedrich L. Bauer of Technical University Munich proposed the idea in 1955 and filed a patent in 1957. The same concept was developed, independently, by the Australian Charles Leonard Hamblin in the first half of 1957.</p>
<h2>Abstract definition</h2>
<p>A stack is a basic computer science data structure and can be defined in an abstract, implementation-free manner, or it can be generally defined as a linear list of items in which all additions and deletion are restricted to one end that is Top.</p>
<p>This is a VDM (Vienna Development Method) description of a stack:</p>
<p>Function signatures:</p>
<p>Generally, the information represented by each node is a record rather than a single data element. However, for sequencing purposes, nodes are compared according to their keys rather than any part of their associated records.</p>
<p>The major advantage of binary search trees over other data structures is that the related sorting algorithms and search algorithms such as in-order traversal can be very efficient. The other advantages are:</p>
<p>Binary search trees are a fundamental data structure used to construct more abstract data structures such as sets, multisets, and associative arrays. Some of their disadvantages are as follows:</p>
<p></p>
<h2>Contents</h2>
<p></p>
<p><b>Quicksort</b>, or <b>partition-exchange sort</b>, is a sorting algorithm developed by Tony Hoare that, on average, makes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare. Quicksort is often faster in practice than other O(n log n) algorithms. Additionally, quicksort's sequential and localized memory references work well with a cache. Quicksort is a comparison sort and, in efficient implementations, is not a stable sort. Quicksort can be implemented with an in-place partitioning algorithm, so the entire sort can be done with only O(log n) additional space used by the stack during the recursion.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>History</h2>
<p>The quicksort algorithm was developed in 1960 by Tony Hoare while in the Soviet Union, as a visiting student at Moscow State University. At that time, Hoare worked in a project on machine translation for the National Physical Laboratory. He developed the algorithm in order to sort the words to be translated, to make them more easily matched to an already-sorted Russian-to-English dictionary that was stored on magnetic tape.</p>
<p>Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort function, hence it lent its name to the C standard library function <strong>qsort</strong> and in the reference implementation of Java. It was analyzed extensively by Robert Sedgewick, who wrote his Ph.D. thesis about the algorithm and suggested several improvements.</p>
<h2>Algorithm</h2>
<p>Heapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Overview</h2>
<p>The heapsort algorithm can be divided into two parts.</p>
<p>In the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based array, the root node is stored at index 0; if <strong>i</strong> is the index of the current node, then</p>
<p><i>
  iParent     = floor((i-1) / 2)
  iLeftChild  = 2*i + 1
  iRightChild = 2*i + 2
</i></p>
<p>In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap. Once all objects have been removed from the heap, the result is a sorted array.</p>
<p>Heapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here. The heap's invariant is preserved after each extraction, so the only cost is that of extraction.</p>
<p>The following is the "simple" way to implement the algorithm in pseudocode. Arrays are <b>zero-based</b> and swap is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at a, while at the end of the sort, the largest element is in a.</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Merge sort</h1>
<p>O(n log n) typical,</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Binary search algorithm</h1>
<p>In computer science, a <b>binary search</b> or <b>half-interval search</b> algorithm finds the position of a specified input value (the search "key") within an array sorted by key value. For binary search, the array should be arranged in ascending or descending order. In each step, the algorithm compares the search key value with the key value of the middle element of the array. If the keys match, then a matching element has been found and its index, or position, is returned. Otherwise, if the search key is less than the middle element's key, then the algorithm repeats its action on the sub-array to the left of the middle element or, if the search key is greater, on the sub-array to the right. If the remaining array to be searched is empty, then the key cannot be found in the array and a special "not found" indication is returned.</p>
<p>Some array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a dynamic array with a fixed maximum size or capacity; Pascal strings are examples of this.</p>
<p>More complicated (non-linear) formulas are occasionally used. For a compact two-dimensional triangular array, for instance, the addressing formula is a polynomial of degree 2.</p>
<h2>Efficiency</h2>
<p>Both store and select take (deterministic worst case) constant time. Arrays take linear (O(n)) space in the number of elements n that they hold.</p>
<p>In an array with element size k and on a machine with a cache line size of B bytes, iterating through an array of n elements requires the minimum of ceiling(nk/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/k better than the number of cache misses needed to access n elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called locality of reference (this does not mean however, that using a perfect hash or trivial hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which can be used to move contiguous blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.</p>
<p>Memory-wise, arrays are compact data structures with no per-element overhead. There may be a per-array overhead, e.g. to store index bounds, but this is language-dependent. It can also happen that elements stored in an array require less memory than the same elements stored in individual variables, because several array elements can be stored in a single word; such arrays are often called packed arrays. An extreme (but commonly used) case is the bit array, where every bit represents a single element. A single octet can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.</p>
<p>Array accesses with statically predictable access patterns are a major source of data parallelism.</p>
<p>Growable arrays are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear (Θ(n)) additional storage, whereas arrays do not reserve additional storage.</p>
<p>Associative arrays provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.</p>
<p>This implementation is used in the heapsort algorithm, where it allows the space in the input array to be reused to store the heap (i.e. the algorithm is done in-place). The implementation is also useful for use as a Priority queue where use of a dynamic array allows insertion of an unbounded number of items.</p>
<p>The upheap/downheap operations can then be stated in terms of an array as follows: suppose that the heap property holds for the indices b, b+1, ..., e. The sift-down function extends the heap property to b−1, b, b+1, ..., e. Only index i = b−1 can violate the heap property. Let j be the index of the largest child of a (for a max-heap, or the smallest child for a min-heap) within the range b, ..., e. (If no such index exists because 2i &gt; e then the heap property holds for the newly extended range and nothing needs to be done.) By swapping the values a and a the heap property for position i is established. At this point, the only problem is that the heap property might not hold for index j. The sift-down function is applied tail-recursively to index j until the heap property is established for all elements.</p>
<p>The sift-down function is fast. In each step it only needs two comparisons and one swap. The index value where it is working doubles in each iteration, so that at most log<sub>2</sub> e steps are required.</p>
<p>For big heaps and using virtual memory, storing elements in an array according to the above scheme is inefficient: (almost) every level is in a different page. B-heaps are binary heaps that keep subtrees in a single page, reducing the number of pages accessed by up to a factor of ten.</p>
<p>The operation of merging two binary heaps takes Θ(n) for equal-sized heaps. The best you can do is (in case of array implementation) simply concatenating the two heap arrays and build a heap of the result. A heap on n elements can be merged with a heap on k elements using O(log n log k) key comparisons, or, in case of a pointer-based implementation, in O(log n log k) time. An algorithm for splitting a heap on n elements into two heaps on k and n-k elements, respectively, based on a new view of heaps as an ordered collections of subheaps was presented in. The algorithm requires O(log n * log n) comparisons. The view also presents a new and conceptually simple algorithm for merging heaps. When merging is a common task, a different heap implementation is recommended, such as binomial heaps, which can be merged in O(log n).</p>
<p>Additionally, a binary heap can be implemented with a traditional binary tree data structure, but there is an issue with finding the adjacent element on the last level on the binary heap when adding an element. This element can be determined algorithmically or by adding extra data to the nodes, called "threading" the tree—instead of merely storing references to the children, we store the inorder successor of the node as well.</p>
<p>It is possible to modify the heap structure to allow extraction of both the smallest and largest element in  time. To do this, the rows alternate between min heap and max heap. The algorithms are roughly the same, but, in each step, one must consider the alternating rows with alternating comparisons. The performance is roughly the same as a normal single direction heap. This idea can be generalised to a min-max-median heap.</p>
<h2>Derivation of index equations</h2>
<p>In an array-based heap, the children and parent of a node can be located via simple arithmetic on the node's index. This section derives the relevant equations for heaps with their root at index 0, with additional notes on heaps with their root at index 1.</p>
<p>To avoid confusion, we'll define the <b>level</b> of a node as its distance from the root, such that the root itself occupies level 0.</p>
<p>The term <b>B-tree</b> may refer to a specific design or it may refer to a general class of designs. In the narrow sense, a B-tree stores keys in its internal nodes but need not store those keys in the records at the leaves. The general class includes variations such as the B+ tree and the B*.</p>
<p>Rudolf Bayer and Ed McCreight invented the B-tree while working at Boeing Research Labs in 1971 (Bayer &amp; McCreight 1972), but they did not explain what, if anything, the B stands for. Douglas Comer explains:</p>
<p>The origin of "B-tree" has never been explained by the authors. As we shall see, "balanced," "broad," or "bushy" might apply. Others suggest that the "B" stands for Boeing. Because of his contributions, however, it seems appropriate to think of B-trees as "Bayer"-trees. (Comer 1979, p. 123 footnote 1)</p>
<p>Donald Knuth speculates on the etymology of B-trees in his May, 1980 lecture on the topic "CS144C classroom lecture about disk storage and B-trees", suggesting the "B" may have originated from Boeing or from Bayer's name.</p>
<p>After a talk at CPM 2013 (24th Annual Symposium on Combinatorial Pattern Matching, Bad Herrenalb, Germany, June 17–19, 2013), Ed McCreight answered a question on B-tree's name by Martin Farach-Colton saying: "Bayer and I were in a lunch time where we get to think a name. And we were, so, B, we were thinking… B is, you know… We were working for Boeing at the time, we couldn't use the name without talking to lawyers. So, there is a B. It has to do with balance, another B. Bayer was the senior author, who did have several years older than I am and had many more publications than I did. So there is another B. And so, at the lunch table we never did resolve whether there was one of those that made more sense than the rest. What really lives to say is: the more you think about what the B in B-trees means, the better you understand B-trees."</p>
<h2>The database problem</h2>
<p>This section describes a problem faced by database designers, outlines a series of increasingly effective solutions to the problem, and ends by describing how the B-tree solves the problem completely.</p>
<p>Usually, sorting and searching algorithms have been characterized by the number of comparison operations that must be performed using order notation. A binary search of a sorted table with  records, for example, can be done in roughly  comparisons. If the table had 1,000,000 records, then a specific record could be located with at most 20 comparisons: .</p>
<p>Large databases have historically been kept on disk drives. The time to read a record on a disk drive far exceeds the time needed to compare keys once the record is available. The time to read a record from a disk drive involves a seek time and a rotational delay. The seek time may be 0 to 20 or more milliseconds, and the rotational delay averages about half the rotation period. For a 7200 RPM drive, the rotation period is 8.33 milliseconds. For a drive such as the Seagate ST3500320NS, the track-to-track seek time is 0.8 milliseconds and the average reading seek time is 8.5 milliseconds. For simplicity, assume reading from disk takes about 10 milliseconds.</p>
<p>Naively, then, the time to locate one record out of a million would take 20 disk reads times 10 milliseconds per disk read, which is 0.2 seconds.</p>
<p>If all keys are known ahead of time, a perfect hash function can be used to create a perfect hash table that has no collisions. If minimal perfect hashing is used, every location in the hash table can be used as well.</p>
<p>Perfect hashing allows for constant time lookups in the worst case. This is in contrast to most chaining and open addressing methods, where the time for lookup is low on average, but may be very large (proportional to the number of entries) for some sets of keys.</p>
<h2>Key statistics</h2>
<p>A critical statistic for a hash table is called the load factor. This is simply the number of entries divided by the number of buckets, that is, n/k where n is the number of entries and k is the number of buckets.</p>
<p>If the load factor is kept reasonable, the hash table should perform well, provided the hashing is good. If the load factor grows too large, the hash table will become slow, or it may fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor is kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries and so does not achieve the desired constant time.</p>
<p>Forwards</p>
<p><i>
node  := someNode
 <b>do</b>
     do something with node.value
     node  := node.next
 <b>while</b> node ≠ someNode
</i></p>
<p>Backwards</p>
<p><i>
node  := someNode
 <b>do</b>
     do something with node.value
     node  := node.prev
 <b>while</b> node ≠ someNode
</i></p>
<p>Notice the postponing of the test to the end of the loop. This is important for the case where the list contains only the single node someNode.</p>
<p>This simple function inserts a node into a doubly-linked circularly linked list after a given element:</p>
<p><i>
<b>function</b> insertAfter(Node node, Node newNode)
     newNode.next  := node.next
     newNode.prev  := node
     node.next.prev  := newNode
     node.next       := newNode
</i></p>
<p>To do an "insertBefore", we can simply "insertAfter(node.prev, newNode)".</p>
<p>Inserting an element in a possibly empty list requires a special function:</p>
<p><i>
<b>function</b> insertEnd(List list, Node node)
     <b>if</b> list.lastNode == <b>null</b>
         node.prev := node
         node.next := node
     <b>else</b>
         insertAfter(list.lastNode, node)
     list.lastNode := node
</i></p>
<p><i>
  init: -&gt; Stack
  push: N x Stack -&gt; Stack
  top: Stack -&gt; (N U ERROR)
  pop: Stack -&gt; Stack
  isempty: Stack -&gt; Boolean
</i></p>
<p>(where N indicates an element (natural numbers in this case), and U indicates set union)</p>
<p>Semantics:</p>
<p><i>
  top(init()) = ERROR
  top(push(i,s)) = i
  pop(init()) = init()
  pop(push(i, s)) = s
  isempty(init()) = true
  isempty(push(i, s)) = false
</i></p>
<h2>Inessential operations</h2>
<p>In many implementations, a stack has more operations than "push" and "pop". An example is "top of stack", or "peek", which observes the top-most element without removing it from the stack. Since this can be done with a "pop" and a "push" with the same data, it is not essential. An underflow condition can occur in the "stack top" operation if the stack is empty, the same as "pop". Also, implementations often have a function which just returns whether the stack is empty.</p>
<h2>Software stacks</h2>
<p>In most high level languages, a stack can be easily implemented either through an array or a linked list. What identifies the data structure as a stack in either case is not the implementation but the interface: the user is only allowed to pop or push items onto the array or linked list, with few other helper operations. The following will demonstrate both implementations, using C.</p>
<p>The <b>array implementation</b> aims to create an array where the first element (usually at the zero-offset) is the bottom. That is, <strong>array</strong> is the first element pushed onto the stack and the last element popped off. The program must keep track of the size, or the length of the stack. The stack itself can therefore be effectively implemented as a two-element structure in C:</p>
<p><i>
typedef struct {
    size_t size;
    int items;
} STACK;
</i></p>
<p>The <strong>push()</strong> operation is used both to initialize the stack, and to store values to it. It is responsible for inserting (copying) the value into the <strong>ps-&gt;items</strong> array and for incrementing the element counter (<strong>ps-&gt;size</strong>). In a responsible C implementation, it is also necessary to check whether the array is already full to prevent an overrun.</p>
<p><i>
void push(STACK *ps, int x)
{
    if (ps-&gt;size == STACKSIZE) {
        fputs("Error: stack overflow\n", stderr);
        abort();
    } else
        ps-&gt;items = x;
}
</i></p>
<h2>Determining whether a tree is a BST or not</h2>
<p>Sometimes we already have a binary tree, and we need to determine whether or not it is a BST. This is an interesting problem which has a simple recursive solution.</p>
<p>The BST property—every node on the right subtree has to be larger than the current node and every node on the left subtree has to be smaller than (or equal to - should not be the case as only unique values should be in the tree - this also poses the question as to if such nodes should be left or right of this parent) the current node—is the key to figuring out whether a tree is a BST or not. On first thought it might look like we can simply traverse the tree, at every node check whether the node contains a value larger than the value at the left child and smaller than the value on the right child, and if this condition holds for all the nodes in the tree then we have a BST. This is the so-called "Greedy approach," making a decision based on local properties. But this approach clearly won't work for the following tree:</p>
<p><i>
     20
    /  \
  10    30
       /  \
      5    40
</i></p>
<p>In the tree above, each node meets the condition that the node contains a value larger than its left child and smaller than its right child hold, and yet it is not a BST: the value 5 is on the right subtree of the node containing 20, a violation of the BST property!</p>
<p>How do we solve this? It turns out that instead of making a decision based solely on the values of a node and its children, we also need information flowing down from the parent as well. In the case of the tree above, if we could remember about the node containing the value 20, we would see that the node with value 5 is violating the BST property contract.</p>
<p>So the condition we need to check at each node is: a) if the node is the left child of its parent, then it must be smaller than (or equal to) the parent and it must pass down the value from its parent to its right subtree to make sure none of the nodes in that subtree is greater than the parent, and similarly b) if the node is the right child of its parent, then it must be larger than the parent and it must pass down the value from its parent to its left subtree to make sure none of the nodes in that subtree is lesser than the parent.</p>
<p>A simple but elegant recursive solution in C++ can explain this further:</p>
<p><i>
struct TreeNode {
    int data;
    TreeNode *left;
    TreeNode *right;
};
 
bool isBST(TreeNode *node, int minData, int maxData) {
    if(node == NULL) return true;
    if(node-&gt;data &lt; minData || node-&gt;data &gt; maxData) return false;
 
    return isBST(node-&gt;left, minData, node-&gt;data) &amp;&amp; isBST(node-&gt;right, node-&gt;data, maxData);
}
</i></p>
<p>The initial call to this function can be something like this:</p>
<p><i>
if(isBST(root, INT_MIN, INT_MAX)) {
    puts("This is a BST.");
} else {
    puts("This is NOT a BST!");
}
</i></p>
<p>Quicksort is a divide and conquer algorithm. Quicksort first divides a large array into two smaller sub-arrays: the low elements and the high elements. Quicksort can then recursively sort the sub-arrays.</p>
<p>The steps are:</p>
<p>The base case of the recursion is arrays of size zero or one, which never need to be sorted. In pseudocode, a quicksort that sorts elements i through k (inclusive) of an array A can be expressed compactly as:171</p>
<p><i>
quicksort(A, i, k):
  if i &lt; k:
    p := partition(A, i, k)
    quicksort(A, i, p - 1)
    quicksort(A, p + 1, k)
</i></p>
<p>Sorting the entire array is accomplished by calling <strong>quicksort(A, 1, length(A))</strong>. The <strong>partition</strong> operation is step 2 from the description in English, above. It can be defined as:</p>
<p><i>
<b>function</b> heapsort(a, count) <b>is</b>
    <b>input:</b> an unordered array a of length count
 
    (Build the heap in array a so that largest value is at the root)
    heapify(a, count)

    (The following loop maintains the invariants that a is a heap and every element
     beyond end is greater than everything before it (so a is in sorted order))
    end ← count - 1
    <b>while</b> end &gt; 0 <b>do</b>
        (a is the root and largest value. The swap moves it in front of the sorted elements.)
        swap(a, a)
        (the heap size is reduced by one)
        end ← end - 1
        (the swap ruined the heap property, so restore it)
        siftDown(a, 0, end)
</i></p>
<p>The sorting routine uses two subroutines, heapify and siftDown. The former is the common in-place heap construction routine, while the second is a common subroutine for implementing heapify.</p>
<p><i>
(Put elements of a in heap order, in-place)
<b>function</b> heapify(a, count) <b>is</b>
    (start is assigned the index in a of the last parent node)
    (the last element in a 0-based array is at index count-1; find the parent of that element )
    start ← floor ((count - 2 ) / 2)
    
    <b>while</b> start ≥ 0 <b>do</b>
        (sift down the node at index start to the proper place such that all nodes below
         the start index are in heap order)
        siftDown(a, start, count-1)
        (go to the next parent node)
        start ← start - 1
    (after sifting down the root all nodes/elements are in heap order)

<b>function</b> siftDown(a, start, end) <b>is</b>
    root ← start

    <b>while</b> root * 2 + 1 ≤ end <b>do</b>    (While the root has at least one child)
        child ← root * 2 + 1       (left child)
        swap ← root                (keeps track of child to swap with)

        <b>if</b> a &lt; a
            swap ← child
        (if there is a right child and that child is greater)
        <b>if</b> child+1 ≤ end <b>and</b> a &lt; a
            swap ← child + 1
        <b>if</b> swap ≠ root
            swap(a, a)
            root ← swap            (repeat to continue sifting down the child now)
        <b>else</b>
            <b>return</b>
</i></p>
<p>The heapify function can be thought of as building a heap from the bottom up, successively shifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be conceptually simpler to grasp. This "siftUp" version can be visualized as starting with an empty heap and successively inserting elements, whereas the "siftDown" version given above treats the entire input array as a full, "broken" heap and "repairs" it starting from the last non-trivial sub-heap (that is, the last parent node).</p>
<p>Also, the "siftDown" version of heapify has O(n) time complexity, while the "siftUp" version given below has O(n log n) time complexity due to its equivalence with inserting each element, one at a time, into an empty heap. This may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never has an impact on asymptotic analysis.</p>
<p>To grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call increases with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more "deep" nodes than there are "shallow" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the "bottom" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call decreases as the depth of the node on which the call is made increases. Thus, when the "siftDown" heapify begins and is calling siftDown on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the "height" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.</p>
<p>The heapsort algorithm itself has O(n log n) time complexity using either version of heapify.</p>
<p><i>
 <b>function</b> heapify(a,count) is
     (end is assigned the index of the first (left) child of the root)
     end := 1
     
     <b>while</b> end &lt; count
         (sift up the node at index end to the proper place such that all nodes above
          the end index are in heap order)
         siftUp(a, 0, end)
         end := end + 1
     (after sifting up the last node all nodes are in heap order)
 
 <b>function</b> siftUp(a, start, end) <b>is</b>
     <b>input: </b> start represents the limit of how far up the heap to sift.
                   end is the node to sift up.
     child := end 
     <b>while</b> child &gt; start
         parent := floor((child-1) / 2)
         <b>if</b> a &lt; a <b>then</b> (out of max-heap order)
             swap(a, a)
             child := parent (repeat to continue sifting up the parent now)
         <b>else</b>
             <b>return</b>
</i></p>
<h2>Variations</h2>
<p>In computer science, <b>merge sort</b> (also commonly spelled <b>mergesort</b>) is an O(n log n) comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and Neumann as early as 1948.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Algorithm</h2>
<p>Conceptually, a merge sort works as follows:</p>
<p>Example C like code using indices for top down merge sort algorithm that recursively splits the list (called runs in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step could be avoided if the recursion alternated between two functions so that the direction of the merge corresponds with the level of recursion.</p>
<p><i>
TopDownMergeSort(A, B, n)
{
    TopDownSplitMerge(A, 0, n, B);
}
 
CopyArray(B, iBegin, iEnd, A)
{
    for(k = iBegin; k &lt; iEnd; k++)
        A = B;
}
 
// iBegin is inclusive; iEnd is exclusive (A is not in the set)
TopDownSplitMerge(A, iBegin, iEnd, B)
{
    if(iEnd - iBegin &lt; 2)                       // if run size == 1
        return;                                 //   consider it sorted
    // recursively split runs into two halves until run size == 1,
    // then merge them and return back up the call chain
    iMiddle = (iEnd + iBegin) / 2;              // iMiddle = mid point
    TopDownSplitMerge(A, iBegin,  iMiddle, B);  // split / merge left  half
    TopDownSplitMerge(A, iMiddle,    iEnd, B);  // split / merge right half
    TopDownMerge(A, iBegin, iMiddle, iEnd, B);  // merge the two half runs
    CopyArray(B, iBegin, iEnd, A);              // copy the merged runs back to A
}
 
//  left half is A
// right half is A
TopDownMerge(A, iBegin, iMiddle, iEnd, B)
{
    i0 = iBegin, i1 = iMiddle;
 
    // While there are elements in the left or right runs
    for (j = iBegin; j &lt; iEnd; j++) {
        // If left run head exists and is &lt;= existing right run head.
        if (i0 &lt; iMiddle &amp;&amp; (i1 &gt;= iEnd || A &lt;= A))
            B = A;
            i0 = i0 + 1;
        else
            B = A;
            i1 = i1 + 1;    }
 
}
</i></p>
<p>Example code for C using indices for bottom up merge sort algorithm which treats the list as an array of n sublists (called runs in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:</p>
<p><i>
/* array A has the items to sort; array B is a work array */
BottomUpSort(int n, int A, int B)
{
  int width;
 
  /* Each 1-element run in A is already "sorted". */
 
  /* Make successively longer sorted runs of length 2, 4, 8, 16... until whole array is sorted. */
  for (width = 1; width &lt; n; width = 2 * width)
    {
      int i;
 
      /* Array A is full of runs of length width. */
      for (i = 0; i &lt; n; i = i + 2 * width)
        {
          /* Merge two runs: A and A to B */
          /* or copy A to B ( if(i+width &gt;= n) ) */
          BottomUpMerge(A, i, min(i+width, n), min(i+2*width, n), B);
        }
 
      /* Now work array B is full of runs of length 2*width. */
      /* Copy array B to array A for next iteration. */
      /* A more efficient implementation would swap the roles of A and B */
      CopyArray(A, B, n);
      /* Now array A is full of runs of length 2*width. */
    }
}
 
BottomUpMerge(int A, int iLeft, int iRight, int iEnd, int B)
{
  int i0 = iLeft;
  int i1 = iRight;
  int j;
 
  /* While there are elements in the left or right lists */
  for (j = iLeft; j &lt; iEnd; j++)
    {
      /* If left list head exists and is &lt;= existing right list head */
      if (i0 &lt; iRight &amp;&amp; (i1 &gt;= iEnd || A &lt;= A))
        {
          B = A;
          i0 = i0 + 1;
        }
      else
        {
          B = A;
          i1 = i1 + 1;
        }
    }
}
</i></p>
<p>A binary search halves the number of items to check with each iteration, so locating an item (or determining its absence) takes logarithmic time. A binary search is a dichotomic divide and conquer search algorithm.</p>
<p></p>
<h2>Contents</h2>
<p></p>
<h2>Overview</h2>
<p>Searching a sorted collection is a common task. A dictionary is a sorted list of word definitions. Given a word, one can find its definition. A telephone book is a sorted list of people's names, addresses, and telephone numbers. Knowing someone's name allows one to quickly find their telephone number and address.</p>
<p>If the list to be searched contains more than a few items (a dozen, say) a binary search will require far fewer comparisons than a linear search, but it imposes the requirement that the list be sorted. Similarly, a hash search can be faster than a binary search but imposes still greater requirements. If the contents of the array are modified between searches, maintaining these requirements may even take more time than the searches. And if it is known that some items will be searched for much more often than others, and it can be arranged so that these items are at the start of the list, then a linear search may be the best.</p>
<p>More generally, algorithm allows searching over argument of any monotonic function for a point, at which function reaches the arbitrary value (enclosed between minimum and maximum at the given range).</p>
<h2>Examples</h2>
<p>Example: The list to be searched: L = 1 3 4 6 8 9 11. The value to be found: X = 4.</p>
<p><i>
   Compare X to 6. X is smaller. Repeat with L = 1 3 4.
   Compare X to 3. X is bigger. Repeat with L = 4.
   Compare X to 4. They are equal. We're done, we found X.
</i></p>
<p>This is called Binary Search: each iteration of (1)-(4) the length of the list we are looking in gets cut in half; therefore, the total number of iterations cannot be greater than logN.</p>
<p>Balanced trees require O(log n) time for indexed access, but also permit inserting or deleting elements in O(log n) time, whereas growable arrays require linear (Θ(n)) time to insert or delete elements at an arbitrary position.</p>
<p>Linked lists allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.</p>
<p>An Iliffe vector is an alternative to a multidimensional array structure. It uses a one-dimensional array of references to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row. Thus an element in row i and column j of an array A would be accessed by double indexing (A in typical notation). This alternative structure allows ragged or jagged arrays, where each row may have a different size — or, in general, where the valid range of each index depends on the values of all preceding indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.<br clear="left"></p>
<p>For a general node located at index  (beginning from 0), we will first derive the index of its right child, .</p>
<p>Let node  be located in level , and note that any level  contains exactly  nodes. Furthermore, there are exactly  nodes contained in the layers up to and including layer  (think of binary arithmetic; 0111...111 = 1000...000 - 1). Because the root is stored at 0, the th node will be stored at index . Putting these observations together yields the following expression for the <b>index of the last node in layer l</b>.</p>
<p>Let there be  nodes after node  in layer L, such that</p>
<p>Each of these  nodes must have exactly 2 children, so there must be  nodes separating 's right child from the end of its layer ().</p>
<p>As required.</p>
<p>Noting that the left child of any node is always 1 place before its right child, we get .</p>
<p>If the root is located at index 1 instead of 0, the last node in each level is instead at index . Using this throughout yields  and  for heaps with their root at 1.</p>
<p>Every node is either the left or right child of its parent, so we know that either of the following is true.</p>
<p>Hence,</p>
<p>The time won't be that bad because individual records are grouped together in a disk <b>block</b>. A disk block might be 16 kilobytes. If each record is 160 bytes, then 100 records could be stored in each block. The disk read time above was actually for an entire block. Once the disk head is in position, one or more disk blocks can be read with little delay. With 100 records per block, the last 6 or so comparisons don't need to do any disk reads—the comparisons are all within the last disk block read.</p>
<p>To speed the search further, the first 13 to 14 comparisons (which each required a disk access) must be sped up.</p>
<p>A significant improvement can be made with an index. In the example above, initial disk reads narrowed the search range by a factor of two. That can be improved substantially by creating an auxiliary index that contains the first record in each disk block (sometimes called a sparse index). This auxiliary index would be 1% of the size of the original database, but it can be searched more quickly. Finding an entry in the auxiliary index would tell us which block to search in the main database; after searching the auxiliary index, we would have to search only that one block of the main database—at a cost of one more disk read. The index would hold 10,000 entries, so it would take at most 14 comparisons. Like the main database, the last 6 or so comparisons in the aux index would be on the same disk block. The index could be searched in about 8 disk reads, and the desired record could be accessed in 9 disk reads.</p>
<p>The trick of creating an auxiliary index can be repeated to make an auxiliary index to the auxiliary index. That would make an aux-aux index that would need only 100 entries and would fit in one disk block.</p>
<p>Instead of reading 14 disk blocks to find the desired record, we only need to read 3 blocks. Reading and searching the first (and only) block of the aux-aux index identifies the relevant block in aux-index. Reading and searching that aux-index block identifies the relevant block in the main database. Instead of 150 milliseconds, we need only 30 milliseconds to get the record.</p>
<p>The auxiliary indices have turned the search problem from a binary search requiring roughly  disk reads to one requiring only  disk reads where  is the blocking factor (the number of entries per block:  entries per block;  reads).</p>
<p>In practice, if the main database is being frequently searched, the aux-aux index and much of the aux index may reside in a disk cache, so they would not incur a disk read.</p>
<p>If the database does not change, then compiling the index is simple to do, and the index need never be changed. If there are changes, then managing the database and its index becomes more complicated.</p>
<p>Deleting records from a database doesn't cause much trouble. The index can stay the same, and the record can just be marked as deleted. The database stays in sorted order. If there is a large number of deletions, then the searching and storage become less efficient.</p>
<p>Insertions can be very slow in a sorted sequential file because room for the inserted record must be made. Inserting a record before the first record in the file requires shifting all of the records down one. Such an operation is just too expensive to be practical. A trick is to leave some space lying around to be used for insertions. Instead of densely storing all the records in a block, the block can have some free space to allow for subsequent insertions. Those records would be marked as if they were "deleted" records.</p>
<p>Both insertions and deletions are fast as long as space is available on a block. If an insertion won't fit on the block, then some free space on some nearby block must be found and the auxiliary indices adjusted. The hope is that enough space is nearby such that a lot of blocks do not need to be reorganized. Alternatively, some out-of-sequence disk blocks may be used.</p>
<p>The B-tree uses all of the ideas described above. In particular, a B-tree:</p>
<p>Second to that, one can examine the variance of number of entries per bucket. For example, two tables both have 1000 entries and 1000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.</p>
<p>A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.</p>
<h2>Collision resolution</h2>
<p>Hash collisions are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the birthday problem there is approximately a 95% chance of at least two of the keys being hashed to the same slot.</p>
<p>Therefore, most hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values.</p>
<p>In the method known as separate chaining, each bucket is independent, and has some sort of list of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation. (The technique is also called open hashing or closed addressing.)</p>
<p>In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries per bucket are not needed or desirable. If these cases happen often, the hashing is not working well, and this needs to be fixed.</p>
<p>Chained hash tables with linked lists are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.</p>
<p>The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is sufficiently uniform, the average cost of a lookup depends only on the average number of keys per bucket—that is, on the load factor.</p>
<p>Chained hash tables remain effective even when the number of table entries n is much higher than the number of slots. Their performance degrades more gracefully (linearly) with the load factor. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list, and possibly even faster than a balanced search tree.</p>
<p>To insert at the beginning we simply "insertAfter(list.lastNode, node)".</p>
<p>Finally, removing a node must deal with the case where the list empties:</p>
<p><i>
<b>function</b> remove(List list, Node node)
     <b>if</b> node.next == node
         list.lastNode := null
     <b>else</b>
         node.next.prev := node.prev
         node.prev.next := node.next
         <b>if</b> node == list.lastNode
             list.lastNode := node.prev;
     <b>destroy</b> node
</i></p>
<p><br></p>
<h2>Advanced concepts</h2>
<p>An asymmetric doubly-linked list is somewhere between the singly-linked list and the regular doubly-linked list. It shares some features with the singly linked list (single-direction traversal) and others from the doubly-linked list (ease of modification)</p>
<p>The <strong>pop()</strong> operation is responsible for removing a value from the stack, and decrementing the value of <strong>ps-&gt;size</strong>. A responsible C implementation will also need to check that the array is not already empty.</p>
<p><i>
int pop(STACK *ps)
{
    if (ps-&gt;size == 0){
        fputs("Error: stack underflow\n", stderr);
        abort();
    } else
        return ps-&gt;items;
}
</i></p>
<p>If we use a dynamic array, then we can implement a stack that can grow or shrink as much as needed. The size of the stack is simply the size of the dynamic array. A dynamic array is a very efficient implementation of a stack, since adding items to or removing items from the end of a dynamic array is amortized O(1) time.</p>
<p>The <b>linked-list</b> implementation is equally simple and straightforward. In fact, a simple singly linked list is sufficient to implement a stack—it only requires that the head node or element can be removed, or popped, and a node can only be inserted by becoming the new head node.</p>
<p>Unlike the array implementation, our structure typedef corresponds not to the entire stack structure, but to a single node:</p>
<p><i>
typedef struct stack {
    int data;
    struct stack *next;
} STACK;
</i></p>
<p>Such a node is identical to a typical singly linked list node, at least to those that are implemented in C.</p>
<p>Essentially we keep creating a valid range (starting from ) and keep shrinking it down for each node as we go down recursively.</p>
<h2>Difference between binary tree and binary search tree</h2>
<p>Binary tree: In short, a binary tree is a tree where each node has up to two leaves. In a binary tree, a left child node and a right child node contain values which can be either greater, less, or equal to parent node.</p>
<p><i>
    3
   / \
  4   5
</i></p>
<p>Binary Search Tree: In binary search tree, the left child contains nodes with values less than the parent node and where the right child only contains nodes with values greater than the parent node. There must be no duplicate nodes.</p>
<p><i>
    4
   / \
  3   5
</i></p>
<h2>Operations</h2>
<p>Operations, such as <b>find</b>, on a binary search tree require comparisons between nodes. These comparisons are made with calls to a comparator, which is a subroutine that computes the total order (linear order) on any two keys. This comparator can be explicitly or implicitly defined, depending on the language in which the binary search tree was implemented. A common comparator is the less-than function, for example, a &lt; b, where a and b are keys of two nodes a and b in a binary search tree.</p>
<p>Searching a binary search tree for a specific key can be a recursive or an iterative process.</p>
<p>We begin by examining the root node. If the tree is null, the key we are searching for does not exist in the tree. Otherwise, if the key equals that of the root, the search is successful and we return the node. If the key is less than that of the root, we search the left subtree. Similarly, if the key is greater than that of the root, we search the right subtree. This process is repeated until the key is found or the remaining subtree is null. If the searched key is not found before a null subtree is reached, then the item must not be present in the tree. This is easily expressed as a recursive algorithm:</p>
<p><i>
<b>function</b> <u>Find-recursive</u>(key, node):  // call initially with node = root
    <b>if</b> node = Null <b>or</b> node.key = key <b>then</b>
        <b>return</b> node
    <b>else if</b> key &lt; node.key <b>then</b>
        <b>return</b> <u>Find-recursive</u>(key, node.left)
    <b>else</b>
        <b>return</b> <u>Find-recursive</u>(key, node.right)
</i></p>
<p><i>
  // left is the index of the leftmost element of the subarray
  // right is the index of the rightmost element of the subarray (inclusive)
  // number of elements in subarray = right-left+1
  partition(array, left, right)
     pivotIndex := choosePivot(array, left, right)
     pivotValue := array
     swap array and array
     storeIndex := left
     <b>for</b> i <b>from</b> left <b>to</b> right - 1
         <b>if</b> array &lt; pivotValue
             swap array and array
             storeIndex := storeIndex + 1
     swap array and array  // Move pivot to its final place
     <b>return</b> storeIndex
</i></p>
<p>This is the in-place partition algorithm. It partitions the portion of the array between indexes left and right, inclusively, by moving all elements less than or equal <strong>array</strong> before the pivot, and the greater elements after it. In the process it also finds the final position for the pivot element, which it returns. It temporarily moves the pivot element to the end of the subarray, so that it doesn't get in the way. Because it only uses exchanges, the final list has the same elements as the original list. Notice that an element may be exchanged multiple times before reaching its final place. Also, in case of pivot duplicates in the input array, they can be spread across the right subarray, in any order. This doesn't represent a partitioning failure, as further sorting will reposition and finally "glue" them together.</p>
<p>This form of the partition algorithm is not the original form; multiple variations can be found in various textbooks, such as versions not having the storeIndex. However, this form is probably the easiest to understand.</p>
<p>Each recursive call to the combined quicksort function reduces the size of the array being sorted by at least one element, since in each invocation the element at pivotNewIndex is placed in its final position. Therefore, this algorithm is guaranteed to terminate after at most n recursive calls. However, since partition reorders elements within a partition, this version of quicksort is not a stable sort.</p>
<p>In very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case. The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by Sedgewick). This "median of three" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known.</p>
<p>Selecting a pivot element is also complicated by the existence of integer overflow. If the boundary indices of the subarray being sorted are sufficiently large, the naïve expression for the middle index, (left + right)/2, will cause overflow and provide an invalid pivot index. This can be overcome by using, for example, left + (right-left)/2 to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element.</p>
<p>With a partitioning algorithm such as the one described above (even with one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the algorithm takes quadratic time to sort an array of equal values.</p>
<p>To solve this quicksort equivalent of the Dutch national flag problem, an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot. (Bentley and McIlroy call this a "fat partition" and note that it was already implemented in the <strong>qsort</strong> of Version 7 Unix.) The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted. In pseudocode, the quicksort algorithm becomes</p>
<p><i>
<b>function</b> quicksort(A, lo, hi)
    if lo &lt; hi
        p = pivot(A, lo, hi)
        left, right = partition(A, p, lo, hi)  // note: multiple return values
        quicksort(A, lo, left)
        quicksort(A, right, hi)
</i></p>
<p>The best case for the algorithm now occurs when all elements are equal (or are chosen from a small set of k ≪ n elements). In the case of all equal elements, the modified quicksort will perform at most two recursive calls on empty subarrays and thus finish in linear time.</p>
<p>Bottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000. This version of heapsort keeps the linear-time heap-building phase, but changes the second phase, as follows. Ordinary heapsort extracts the top of the heap, a, and fills the gap it leaves with a, then sifts this latter element down the heap; but this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. Bottom-up heapsort instead finds the element to fill the gap, by tracing a path of maximum children down the heap as before, but then sifts that element up the heap, which is likely to take fewer steps.</p>
<p><i>
<b>function</b> leafSearch(a, end, i) <b>is</b>
    j ← i
    <b>while</b> 2×j ≤ end <b>do</b>
        (Determine which of j's children is the greater)
        <b>if</b> 2×j+1 &lt; end <b>and</b> a &gt; a <b>then</b>
            j ← 2×j+1
        <b>else</b>
            j ← 2×j
    <b>return</b> j
</i></p>
<p>The return value of the <strong>leafSearch</strong> is used in a replacement for the <strong>siftDown</strong> routine:</p>
<p><i>
<b>function</b> siftDown(a, end, i) <b>is</b>
    j ← leafSearch(a, end, i)
    <b>while</b> a &gt; a <b>do</b>
        j ← parent(j)
    x ← a
    a ← a
    <b>while</b> j &gt; i <b>do</b>
        swap x, a
        j ← parent(j)
</i></p>
<p>Bottom-up heapsort requires only 1.5 n log n + O(n) comparisons in the worst case and n log n + O(1) on average. A 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort, though, presumably because modern branch prediction nullifies the cost of the comparisons that bottom-up heapsort manages to avoid.</p>
<p>Pseudocode for top down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.</p>
<p><i>
<b>function</b> merge_sort(list m)
    // Base case. A list of zero or one elements is sorted, by definition.
    <b>if</b> length(m) &lt;= 1
        <b>return</b> m

    // Recursive case. First, *divide* the list into equal-sized sublists.
    <b>var</b> list left, right
    <b>var</b> integer middle = length(m) / 2
    <b>for each</b> x <b>in</b> m <b>before</b> middle
         add x to left
    <b>for each</b> x <b>in</b> m <b>after or equal</b> middle
         add x to right

    // Recursively sort both sublists.
    left = merge_sort(left)
    right = merge_sort(right)
    // *Conquer*: merge the now-sorted sublists.
    <b>return</b> merge(left, right)
</i></p>
<p>In this example, the <strong>merge</strong> function merges the left and right sublists.</p>
<p><i>
<b>function</b> merge(left, right)
    <b>var</b> list result
    // assign the element of the sublists to 'result' variable until there is no element to merge. 
    <b>while</b> length(left) &gt; 0 <b>or</b> length(right) &gt; 0
        <b>if</b> length(left) &gt; 0 <b>and</b> length(right) &gt; 0
           // compare the first two element, which is the small one, of each two sublists.
            <b>if</b> first(left) &lt;= first(right)
                append first(left) to result
                left = rest(left)
            <b>else</b>
                append first(right) to result
                right = rest(right)
        <b>else if</b> length(left) &gt; 0
            append first(left) to result
            left = rest(left)
        <b>else if</b> length(right) &gt; 0
            append first(right) to result
            right = rest(right)
    <b>end while</b>
    <b>return</b> result
</i></p>
<h2>Natural merge sort</h2>
<p>A natural merge sort is similar to a bottom up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. In the bottom up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. Example:</p>
<p><i>
Start       : 3--4--2--1--7--5--8--9--0--6
Select runs : 3--4  2  1--7  5--8--9  0--6
Merge       : 2--3--4  1--5--7--8--9  0--6
Merge       : 1--2--3--4--5--7--8--9  0--6
Merge       : 0--1--2--3--4--5--6--7--8--9
</i></p>
<p>Tournament replacement selection sorts are used to gather the initial runs for external sorting algorithms.</p>
<p>This rather simple game begins something like "I'm thinking of an integer between forty and sixty inclusive, and to your guesses I'll respond 'Higher', 'Lower', or 'Yes!' as might be the case." Supposing that N is the number of possible values (here, twenty-one, as "inclusive" was stated), then at most  questions are required to determine the number, since each question halves the search space. Note that one less question (iteration) is required than for the general algorithm, since the number is already constrained to be within a particular range.</p>
<p>Even if the number to guess can be arbitrarily large, in which case there is no upper bound N, the number can be found in at most  steps (where k is the (unknown) selected number) by first finding an upper bound with one-sided binary search. For example, if the number were 11, the following sequence of guesses could be used to find it: 1 (Higher), 2 (Higher), 4 (Higher), 8 (Higher), 16 (Lower), 12 (Lower), 10 (Higher). Now we know that the number must be 11 because it is higher than 10 and lower than 12.</p>
<p>One could also extend the method to include negative numbers; for example the following guesses could be used to find −13: 0, −1, −2, −4, −8, −16, −12, −14. Now we know that the number must be −13 because it is lower than −12 and higher than −14.</p>
<p>People typically use a mixture of the binary search and interpolative search algorithms when searching a telephone book, after the initial guess we exploit the fact that the entries are sorted and can rapidly find the required entry. For example when searching for Smith, if Rogers and Thomas have been found, one can flip to a page about halfway between the previous guesses. If this shows Samson, it can be concluded that Smith is somewhere between the Samson and Thomas pages so these can be divided.</p>
<p>Even if we do not know a fixed range the number k falls in, we can still determine its value by asking  simple yes/no questions of the form "Is k greater than x?" for some number x. As a simple consequence of this, if you can answer the question "Is this integer property k greater than a given value?" in some amount of time then you can find the value of that property in the same amount of time with an added factor of . This is called a reduction, and it is because of this kind of reduction that most complexity theorists concentrate on decision problems, algorithms that produce a simple yes/no answer.</p>
<p>For example, suppose we could answer "Does this n x n matrix have permanent larger than k?" in O(n2) time. Then, by using binary search, we could find the (ceiling of the) permanent itself in O(n2 log p) time, where p is the value of the permanent. Notice that p is not the size of the input, but the value of the output; given a matrix whose maximum item (in absolute value) is m, p is bounded by . Hence log p = O(n log n + log m). A binary search could find the permanent in O(n3 log n + n2 log m).</p>
<h2>Algorithm</h2>
<p>A straightforward implementation of binary search is recursive. The initial call uses the indices of the entire array to be searched. The procedure then calculates an index midway between the two indices, determines which of the two subarrays to search, and then does a recursive call to search that subarray. Each of the calls is tail recursive, so a compiler need not make a new stack frame for each call. The variables <strong>imin</strong> and <strong>imax</strong> are the lowest and highest inclusive indices that are searched.</p>
<p><i>
int binary_search(int A, int key, int imin, int imax)
{
  // test if array is empty
  if (imax &lt; imin)
    // set is empty, so return value showing not found
    return KEY_NOT_FOUND;
  else
    {
      // calculate midpoint to cut set in half
      int imid = midpoint(imin, imax);
 
      // three-way comparison
      if (A &gt; key)
        // key is in lower subset
        return binary_search(A, key, imin, imid-1);
      else if (A &lt; key)
        // key is in upper subset
        return binary_search(A, key, imid+1, imax);
      else
        // key has been found
        return imid;
    }
}
</i></p>
<p>It is invoked with initial <strong>imin</strong> and <strong>imax</strong> values of <strong>0</strong> and <strong>N-1</strong> for a zero based array of length N.</p>
<h2>Dimension</h2>
<p>The dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array a rectangle of data, a three-dimensional array a block of data, etc.</p>
<p>This should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.</p>
<h2>See also</h2>
<h2>References</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>Now consider the expression .</p>
<p>If node  is a left child, this gives the result immediately, however, it also gives the correct result if node  is a right child. In this case,  must be even, and hence  must be odd.</p>
<p>Therefore, irrespective of whether a node is a left or right child, its parent can be found by the expression:</p>
<h2>See also</h2>
<h2>Notes</h2>
<h2>References</h2>
<h2>External links</h2>
<p>In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.</p>
<h2>Technical description</h2>
<p>Unfortunately, the literature on B-trees is not uniform in its terminology (Folk &amp; Zoellick 1992, p. 362).</p>
<p>Bayer &amp; McCreight (1972), Comer (1979), and others define the <b>order</b> of B-tree as the minimum number of keys in a non-root node. Folk &amp; Zoellick (1992) points out that terminology is ambiguous because the maximum number of keys is not clear. An order 3 B-tree might hold a maximum of 6 keys or a maximum of 7 keys. Knuth (1998, p. 483) avoids the problem by defining the <b>order</b> to be maximum number of children (which is one more than the maximum number of keys).</p>
<p>The term <b>leaf</b> is also inconsistent. Bayer &amp; McCreight (1972) considered the leaf level to be the lowest level of keys, but Knuth considered the leaf level to be one level below the lowest keys (Folk &amp; Zoellick 1992, p. 363). There are many possible implementation choices. In some designs, the leaves may hold the entire data record; in other designs, the leaves may only hold pointers to the data record. Those choices are not fundamental to the idea of a B-tree.</p>
<p>There are also unfortunate choices like using the variable k to represent the number of children when k could be confused with the number of keys.</p>
<p>For simplicity, most authors assume there are a fixed number of keys that fit in a node. The basic assumption is the key size is fixed and the node size is fixed. In practice, variable length keys may be employed (Folk &amp; Zoellick 1992, p. 379).</p>
<p>According to Knuth's definition, a B-tree of order m is a tree which satisfies the following properties:</p>
<p>For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number n of entries in the table.</p>
<p>The bucket chains are often implemented as ordered lists, sorted by the key field; this choice approximately halves the average cost of unsuccessful lookups, compared to an unordered list. However, if some keys are much more likely to come up than others, an unordered list with move-to-front heuristic may be more effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.</p>
<p>Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the <strong>next</strong> pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor cache performance, making the processor cache ineffective.</p>
<p>Some chaining implementations store the first record of each chain in the slot array itself. The number of pointer traversals is decreased by one for most cases. The purpose is to increase cache efficiency of hash table access.</p>
<p>The disadvantage is that an empty bucket takes the same space as a bucket with one entry. To save space, such hash tables often have about as many slots as stored entries, meaning that many slots have two or more entries.</p>
<p>Instead of a list, one can use any other data structure that supports the required operations. For example, by using a self-balancing tree, the theoretical worst-case time of common hash table operations (insertion, deletion, lookup) can be brought down to O(log n) rather than O(n). However, this approach is only worth the trouble and extra memory cost if long delays must be avoided at all costs (e.g. in a real-time application), or if one must guard against many entries hashed to the same slot (e.g. if one expects extremely non-uniform distributions, or in the case of web sites or other publicly accessible services, which are vulnerable to malicious key distributions in requests).</p>
<p>The variant called array hash table uses a dynamic array to store all the entries that hash to the same slot. Each newly inserted entry gets appended to the end of the dynamic array that is assigned to the slot. The dynamic array is resized in an exact-fit manner, meaning it is grown only by as many bytes as needed. Alternative techniques such as growing the array by block sizes or pages were found to improve insertion performance, but at a cost in space. This variation makes more efficient use of CPU caching and the translation lookaside buffer (TLB), because slot entries are stored in sequential memory positions. It also dispenses with the <strong>next</strong> pointers that are required by linked lists, which saves space. Despite frequent array resizing, space overheads incurred by operating system such as memory fragmentation, were found to be small.</p>
<p>An elaboration on this approach is the so-called dynamic perfect hashing, where a bucket that contains k entries is organized as a perfect hash table with k2 slots. While it uses more memory (n2 slots for n entries, in the worst case and n*k slots in the average case), this variant has guaranteed constant worst-case lookup time, and low amortized time for insertion.</p>
<p>In another strategy, called open addressing, all entry records are stored in the bucket array itself. When a new entry has to be inserted, the buckets are examined, starting with the hashed-to slot and proceeding in some probe sequence, until an unoccupied slot is found. When searching for an entry, the buckets are scanned in the same sequence, until either the target record is found, or an unused array slot is found, which indicates that there is no such key in the table. The name "open addressing" refers to the fact that the location ("address") of the item is not determined by its hash value. (This method is also called <b>closed hashing</b>; it should not be confused with "open hashing" or "closed addressing" that usually mean separate chaining.)</p>
<p>It is a list where each node's previous link points not to the previous node, but to the link to itself. While this makes little difference between nodes (it just points to an offset within the previous node), it changes the head of the list: It allows the first node to modify the firstNode link easily.</p>
<p>As long as a node is in a list, its previous link is never null.</p>
<p>To insert a node before another, we change the link that pointed to the old node, using the prev link; then set the new node's next link to point to the old node, and change that node's prev link accordingly.</p>
<p><i>
<b>function</b> insertBefore(Node node, Node newNode)
     <b>if</b> node.prev == <b>null</b>
          <b>error</b> "The node is not in a list"
     newNode.prev  := node.prev
     atAddress(newNode.prev)  := newNode
     newNode.next  := node
     node.prev = addressOf(newNode.next)
</i></p>
<p><i>
<b>function</b> insertAfter(Node node, Node newNode)
     newNode.next  := node.next
     <b>if</b> newNode.next != <b>null</b>
         newNode.next.prev = addressOf(newNode.next)
     node.next  := newNode
     newNode.prev  := addressOf(node.next)
</i></p>
<p>To remove a node, we simply modify the link pointed by prev, regardless of whether the node was the first one of the list.</p>
<p><i>
<b>function</b> remove(Node node)
     atAddress(node.prev)  := node.next
     <b>if</b> node.next != <b>null</b>
         node.next.prev = node.prev
     <b>destroy</b> node
</i></p>
<h2>See also</h2>
<h2>References</h2>
<h2>Navigation menu</h2>
<p>The <strong>push()</strong> operation both initializes an empty stack, and adds a new node to a non-empty one. It works by receiving a data value to push onto the stack, along with a target stack, creating a new node by allocating memory for it, and then inserting it into a linked list as the new head:</p>
<p><i>
void push(STACK **head, int value)
{
    STACK *node = malloc(sizeof(STACK));  /* create a new node */
 
    if (node == NULL){
        fputs("Error: no space available for node\n", stderr);
        abort();
    } else {                                      /* initialize node */
        node-&gt;data = value;
        node-&gt;next = empty(*head) ? NULL : *head; /* insert new head if any */
        *head = node;
    }
}
</i></p>
<p>A <strong>pop()</strong> operation removes the head from the linked list, and assigns the pointer to the head to the previous second node. It checks whether the list is empty before popping from it:</p>
<p><i>
int pop(STACK **head)
{
    if (empty(*head)) {                          /* stack is empty */
       fputs("Error: stack underflow\n", stderr);
       abort();
    } else {                                     //pop a node
        STACK *top = *head;
        int value = top-&gt;data;
        *head = top-&gt;next;
        free(top);
        return value;
    }
}
</i></p>
<p>Some languages, like Perl, LISP and Python, do not call for stack implementations, since <b>push</b> and <b>pop</b> functions are available for any list. All Forth-like languages (such as Adobe PostScript) are also designed around language-defined stacks that are directly visible to and manipulated by the programmer. Examples from Common Lisp:</p>
<p>The same algorithm can be implemented iteratively:</p>
<p><i>
<b>function</b> <u>Find</u>(key, root):
    current-node := root
    <b>while</b> current-node <b>is not Null do</b>
        <b>if</b> current-node.key = key <b>then</b>
            <b>return</b> current-node
        <b>else if</b> key &lt; current-node.key <b>then</b>
            current-node := current-node.left
        <b>else</b>
            current-node := current-node.right
    <b>return Null</b>
</i></p>
<p>Because in the worst case this algorithm must search from the root of the tree to the leaf farthest from the root, the search operation takes time proportional to the tree's height (see tree terminology). On average, binary search trees with n nodes have O(log n) height. However, in the worst case, binary search trees can have O(n) height, when the unbalanced tree resembles a linked list (degenerate tree).</p>
<p>Insertion begins as a search would begin; if the key is not equal to that of the root, we search the left or right subtrees as before. Eventually, we will reach an external node and add the new key-value pair (here encoded as a record 'newNode') as its right or left child, depending on the node's key. In other words, we examine the root and recursively insert the new node to the left subtree if its key is less than that of the root, or the right subtree if its key is greater than or equal to the root.</p>
<p>Here's how a typical binary search tree insertion might be performed in a binary tree in C++:</p>
<p><i>
void insert(Node*&amp; root, int data) {
  if (!root) 
    root = new Node(data);
  else if (data &lt; root-&gt;data)
    insert(root-&gt;left, data);
  else if (data &gt; root-&gt;data)
    insert(root-&gt;right, data);
}
</i></p>
<p>The above destructive procedural variant modifies the tree in place. It uses only constant heap space (and the iterative version uses constant stack space as well), but the prior version of the tree is lost. Alternatively, as in the following Python example, we can reconstruct all ancestors of the inserted node; any reference to the original tree root remains valid, making the tree a persistent data structure:</p>
<p>Two other important optimizations, also suggested by Sedgewick and widely used in practice are:</p>
<p>Quicksort's divide-and-conquer formulation makes it amenable to parallelization using task parallelism. The partitioning step is accomplished through the use of a parallel prefix sum algorithm to compute an index for each array element in its section of the partitioned array. Given an array of size n, the partitioning step performs O(n) work in O(log n) time and requires O(n) additional scratch space. After the array has been partitioned, the two partitions can be sorted recursively in parallel. Assuming an ideal choice of pivots, parallel quicksort sorts an array of size n in O(n log n) work in O(log² n) time using O(n) additional space.</p>
<p>Quicksort has some disadvantages when compared to alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult to parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.</p>
<p>Other more sophisticated parallel sorting algorithms can achieve even better time bounds. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW PRAM with n processors by performing partitioning implicitly.</p>
<h2>Formal analysis</h2>
<p>To sort an array of n distinct elements, quicksort takes O(n log n) time in expection, averaged over all n! permutations of n elements with equal probability. Why? For a start, it is not hard to see that the partition operation takes O(n) time.</p>
<p>In the most unbalanced case, each time we perform a partition we divide the list into two sublists of size 0 and n − 1 (for example, if all elements of the array are equal). This means each recursive call processes a list of size one less than the previous list. Consequently, we can make n − 1 nested calls before we reach a list of size 1. This means that the call tree is a linear chain of n − 1 nested calls. The ith call does O(n − i) work to do the partition, and , so in that case Quicksort takes O(n²) time. That is the worst case: given knowledge of which comparisons are performed by the sort, there are adaptive algorithms that are effective at generating worst-case input for quicksort on-the-fly, regardless of the pivot selection strategy.</p>
<p>In the most balanced case, each time we perform a partition we divide the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, we can make only log₂ n nested calls before we reach a list of size 1. This means that the depth of the call tree is log₂ n. But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only O(n) time all together (each call has some constant overhead, but since there are only O(n) calls at each level, this is subsumed in the O(n) factor). The result is that the algorithm uses only O(n log n) time.</p>
<p>In fact, it's not necessary to be perfectly balanced; even if each pivot splits the elements with 75% on one side and 25% on the other side (or any other fixed fraction), the call depth is still limited to , so the total running time is still O(n log n).</p>
<p>So what happens on average? If the pivot has rank somewhere in the middle 50 percent, that is, between the 25th percentile and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side. If we could consistently choose a pivot from the two middle 50 percent, we would only have to split the list at most  times before reaching lists of size 1, yielding an O(n log n) algorithm.</p>
<h2>Comparison with other sorts</h2>
<p>Heapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm.</p>
<p>Quicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is O(n2), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See quicksort for a detailed discussion of this problem and possible solutions.</p>
<p>Thus, because of the O(n log n) upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort.</p>
<p>Heapsort also competes with merge sort, which has the same time bounds. Merge sort requires Ω(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches. On the other hand, merge sort has several advantages over heapsort:</p>
<p>Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.</p>
<h2>Example</h2>
<p>Let { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)</p>
<p><b>1. Build the heap</b></p>
<p><b>2. Sorting.</b></p>
<h2>Notes</h2>
<h2>Analysis</h2>
<p>In sorting n objects, merge sort has an average and worst-case performance of O(n log n). If the running time of merge sort for a list of length n is T(n), then the recurrence T(n) = 2T(n/2) + n follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the n steps taken to merge the resulting two lists). The closed form follows from the master theorem.</p>
<p>In the worst case, the number of comparisons merge sort makes is equal to or slightly smaller than (n ⌈lg n⌉ - 2⌈lg n⌉ + 1), which is between (n lg n - n + 1) and (n lg n + n + O(lg n)).</p>
<p>For large n and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches α·n fewer than the worst case where </p>
<p>In the worst case, merge sort does about 39% fewer comparisons than quicksort does in the average case. In terms of moves, merge sort's worst case complexity is O(n log n)—the same complexity as quicksort's best case, and merge sort's best case takes about half as many iterations as the worst case.</p>
<p>Merge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.</p>
<p>The number type "int" shown in the code has an influence on how the midpoint calculation can be implemented correctly. With unlimited numbers, the midpoint can be calculated as <strong>"(imin + imax) / 2"</strong>. In practical programming, however, the calculation is often performed with numbers of a limited range, and then the intermediate result <strong>"(imin + imax)"</strong> might overflow. With limited numbers, the midpoint can be calculated correctly as <strong>"imin + ((imax - imin) / 2)"</strong>.</p>
<p>The binary search algorithm can also be expressed iteratively with two index limits that progressively narrow the search range.</p>
<p><i>
int binary_search(int A, int key, int imin, int imax)
{
  // continue searching while  is not empty
  while (imax &gt;= imin)
    {
      // calculate the midpoint for roughly equal partition
      int imid = midpoint(imin, imax);
      if(A == key)
        // key found at index imid
        return imid; 
      // determine which subarray to search
      else if (A &lt; key)
        // change min index to search upper subarray
        imin = imid + 1;
      else         
        // change max index to search lower subarray
        imax = imid - 1;
    }
  // key was not found
  return KEY_NOT_FOUND;
}
</i></p>
<p>The above iterative and recursive versions take three paths based on the key comparison: one path for less than, one path for greater than, and one path for equality. (There are two conditional branches.) The path for equality is taken only when the record is finally matched, so it is rarely taken. That branch path can be moved outside the search loop in the deferred test for equality version of the algorithm. The following algorithm uses only one conditional branch per iteration.</p>
<p><i>
// inclusive indices
//   0 &lt;= imin when using truncate toward zero divide
//     imid = (imin+imax)/2;
//   imin unrestricted when using truncate toward minus infinity divide
//     imid = (imin+imax)&gt;&gt;1; or
//     imid = (int)floor((imin+imax)/2.0);
int binary_search(int A, int key, int imin, int imax)
{
  // continually narrow search until just one element remains
  while (imin &lt; imax)
    {
      int imid = midpoint(imin, imax);
 
      // code must guarantee the interval is reduced at each iteration
      assert(imid &lt; imax);
      // note: 0 &lt;= imin &lt; imax implies imid will always be less than imax
 
      // reduce the search
      if (A &lt; key)
        imin = imid + 1;
      else
        imax = imid;
    }
  // At exit of while:
  //   if A is empty, then imax &lt; imin
  //   otherwise imax == imin
 
  // deferred test for equality
  if ((imax == imin) &amp;&amp; (A == key))
    return imin;
  else
    return KEY_NOT_FOUND;
}
</i></p>
<p>The deferred detection approach foregoes the possibility of early termination on discovery of a match, so the search will take about log<sub>2</sub>(N) iterations. On average, a successful early termination search will not save many iterations. For large arrays that are a power of 2, the savings is about two iterations. Half the time, a match is found with one iteration left to go; one quarter the time with two iterations left, one eighth with three iterations, and so forth. The infinite series sum is 2.</p>
<p>The deferred detection algorithm has the advantage that if the keys are not unique, it returns the smallest index (the starting index) of the region where elements have the search key. The early termination version would return the first match it found, and that match might be anywhere in region of equal keys.</p>
<h2>Performance</h2>
<h2>Navigation menu</h2>
<p>Each internal node’s keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: a<sub>1</sub> and a<sub>2</sub>. All values in the leftmost subtree will be less than a<sub>1</sub>, all values in the middle subtree will be between a<sub>1</sub> and a<sub>2</sub>, and all values in the rightmost subtree will be greater than a<sub>2</sub>.</p>
<p>A B-tree of depth n+1 can hold about U times as many items as a B-tree of depth n, but the cost of search, insert, and delete operations grows with the depth of the tree. As with any balanced tree, the cost grows much more slowly than the number of elements.</p>
<p>Some balanced trees store values only at leaf nodes, and use different kinds of nodes for leaf nodes and internal nodes. B-trees keep values in every node in the tree, and may use the same structure for all nodes. However, since leaf nodes never have children, the B-trees benefit from improved performance if they use a specialized structure.</p>
<h2>Best case and worst case heights</h2>
<p>Let h be the height of the classic B-tree. Let n &gt; 0 be the number of entries in the tree. Let m be the maximum number of children a node can have. Each node can have at most m−1 keys.</p>
<p>It can be shown (by induction for example) that a B-tree of height h with all its nodes completely filled has n=mh−1 entries. Hence, the best case height of a B-tree is:</p>
<p>Well-known probe sequences include:</p>
<p>A drawback of all these open addressing schemes is that the number of stored entries cannot exceed the number of slots in the bucket array. In fact, even with good hash functions, their performance dramatically degrades when the load factor grows beyond 0.7 or so. For many applications, these restrictions mandate the use of dynamic resizing, with its attendant costs.</p>
<p>Open addressing schemes also put more stringent requirements on the hash function: besides distributing the keys more uniformly over the buckets, the function must also minimize the clustering of hash values that are consecutive in the probe order. Using separate chaining, the only concern is that too many objects map to the same hash value; whether they are adjacent or nearby is completely irrelevant.</p>
<p>Open addressing only saves memory if the entries are small (less than four times the size of a pointer) and the load factor is not too small. If the load factor is close to zero (that is, there are far more buckets than stored entries), open addressing is wasteful even if each entry is just two words.</p>
<p>Open addressing avoids the time overhead of allocating each new entry record, and can be implemented even in the absence of a memory allocator. It also avoids the extra indirection required to access the first entry of each bucket (that is, usually the only one). It also has better locality of reference, particularly with linear probing. With small record sizes, these factors can yield better performance than chaining, particularly for lookups. Hash tables with open addressing are also easier to serialize, because they do not use pointers.</p>
<p>On the other hand, normal open addressing is a poor choice for large elements, because these elements fill entire CPU cache lines (negating the cache advantage), and a large amount of space is wasted on large empty table slots. If the open addressing table only stores references to elements (external storage), it uses space comparable to chaining even for large records but loses its speed advantage.</p>
<p>Generally speaking, open addressing is better used for hash tables with small records that can be stored within the table (internal storage) and fit in a cache line. They are particularly suitable for elements of one word or less. If the table is expected to have a high load factor, the records are large, or the data is variable-sized, chained hash tables often perform as well or better.</p>
<p><i>
(setf list (list 'a 'b 'c))
;; ⇒ (A B C)
(pop list)
;; ⇒ A
list
;; ⇒ (B C)
(push 'new list)
;; ⇒ (NEW B C)
</i></p>
<p>C++'s Standard Template Library provides a "<strong>stack</strong>" templated class which is restricted to only push/pop operations. Java's library contains a <strong>Stack</strong> class that is a specialization of <strong>Vector</strong>---this could be considered a design flaw, since the inherited get() method from <strong>Vector</strong> ignores the LIFO constraint of the <strong>Stack</strong>. PHP has an SplStack class.</p>
<h2>Hardware stacks</h2>
<p>A common use of stacks at the architecture level is as a means of allocating and accessing memory.</p>
<p>A typical stack is an area of computer memory with a fixed origin and a variable size. Initially the size of the stack is zero. A stack pointer, usually in the form of a hardware register, points to the most recently referenced location on the stack; when the stack has a size of zero, the stack pointer points to the origin of the stack.</p>
<p>The two operations applicable to all stacks are:</p>
<p>There are many variations on the basic principle of stack operations. Every stack has a fixed location in memory at which it begins. As data items are added to the stack, the stack pointer is displaced to indicate the current extent of the stack, which expands away from the origin.</p>
<p>Stack pointers may point to the origin of a stack or to a limited range of addresses either above or below the origin (depending on the direction in which the stack grows); however, the stack pointer cannot cross the origin of the stack. In other words, if the origin of the stack is at address 1000 and the stack grows downwards (towards addresses 999, 998, and so on), the stack pointer must never be incremented beyond 1000 (to 1001, 1002, etc.). If a pop operation on the stack causes the stack pointer to move past the origin of the stack, a stack underflow occurs. If a push operation causes the stack pointer to increment or decrement beyond the maximum extent of the stack, a stack overflow occurs.</p>
<p>Some environments that rely heavily on stacks may provide additional operations, for example:</p>
<p>Stacks are often visualized growing from the bottom up (like real-world stacks). They may also be visualized growing from left to right, so that "topmost" becomes "rightmost", or even growing from top to bottom. The important feature is that the top of the stack is in a fixed position. The image above and to the right is an example of a top to bottom growth visualization: the top (28) is the stack 'bottom', since the stack 'top' is where items are pushed or popped from. Sometimes stacks are also visualized metaphorically, such as coin holders or Pez dispensers.</p>
<p><i>
 def binary_tree_insert(node, key, value):
     if node is None:
         return TreeNode(None, key, value, None)
     if key == node.key:
         return TreeNode(node.left, key, value, node.right)
     if key &lt; node.key:
         return TreeNode(binary_tree_insert(node.left, key, value), node.key, node.value, node.right)
     else:
         return TreeNode(node.left, node.key, node.value, binary_tree_insert(node.right, key, value))
</i></p>
<p>The part that is rebuilt uses O(log n) space in the average case and O(n) in the worst case (see big-O notation).</p>
<p>In either version, this operation requires time proportional to the height of the tree in the worst case, which is O(log n) time in the average case over all trees, but O(n) time in the worst case.</p>
<p>Another way to explain insertion is that in order to insert a new node in the tree, its key is first compared with that of the root. If its key is less than the root's, it is then compared with the key of the root's left child. If its key is greater, it is compared with the root's right child. This process continues, until the new node is compared with a leaf node, and then it is added as this node's right or left child, depending on its key.</p>
<p>There are other ways of inserting nodes into a binary tree, but this is the only way of inserting nodes at the leaves and at the same time preserving the BST structure.</p>
<p>There are three possible cases to consider:</p>
<p>Broadly speaking, nodes with children are harder to delete. As with all binary trees, a node's in-order successor is its right subtree's left-most child, and a node's in-order predecessor is the left subtree's right-most child. In either case, this node will have zero or one children. Delete it according to one of the two simpler cases above.</p>
<p>Consistently using the in-order successor or the in-order predecessor for every instance of the two-child case can lead to an unbalanced tree, so some implementations select one or the other at different times.</p>
<p>Runtime analysis: Although this operation does not always traverse the tree down to a leaf, this is always a possibility; thus in the worst case it requires time proportional to the height of the tree. It does not require more even when the node has two children, since it still follows a single path and does not visit any node twice.</p>
<p><i>
def find_min(self):   # Gets minimum node (leftmost leaf) in a subtree
    current_node = self
    while current_node.left_child:
        current_node = current_node.left_child
    return current_node
 
def replace_node_in_parent(self, new_value=None):
    if self.parent:
        if self == self.parent.left_child:
            self.parent.left_child = new_value
        else:
            self.parent.right_child = new_value
    if new_value:
        new_value.parent = self.parent
 
def binary_tree_delete(self, key):
    if key &lt; self.key:
        self.left_child.binary_tree_delete(key)
    elif key &gt; self.key:
        self.right_child.binary_tree_delete(key)
    else: # delete the key here
        if self.left_child and self.right_child: # if both children are present
            successor = self.right_child.find_min()
            self.key = successor.key
            successor.binary_tree_delete(successor.key)
        elif self.left_child:   # if the node has only a *left* child
            self.replace_node_in_parent(self.left_child)
        elif self.right_child:  # if the node has only a *right* child
            self.replace_node_in_parent(self.right_child)
        else: # this node has no children
            self.replace_node_in_parent(None)
</i></p>
<p>Once the binary search tree has been created, its elements can be retrieved in-order by recursively traversing the left subtree of the root node, accessing the node itself, then recursively traversing the right subtree of the node, continuing this pattern with each node in the tree as it's recursively accessed. As with all binary trees, one may conduct a pre-order traversal or a post-order traversal, but neither are likely to be useful for binary search trees. An in-order traversal of a binary search tree will always result in a sorted list of node items (numbers, strings or other comparable items).</p>
<p>When the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent. However, when we start from a random permutation, in each recursive call the pivot has a random rank in its list, and so it is in the middle 50 percent about half the time. That is good enough. Imagine that you flip a coin: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't. Imagine that you are flipping a coin over and over until you get k heads. Although this could take a long time, on average only 2k flips are required, and the chance that you won't get k heads after 100k flips is highly improbable (this can be made rigorous using Chernoff bounds). By the same argument, Quicksort's recursion will terminate on average at a call depth of only . But if its average call depth is O(log n), and each level of the call tree processes at most n elements, the total amount of work done on average is the product, O(n log n). Note that the algorithm does not have to verify that the pivot is in the middle half—if we hit it any constant fraction of the times, that is enough for the desired complexity.</p>
<p>An alternative approach is to set up a recurrence relation for the T(n) factor, the time needed to sort a list of size n. In the most unbalanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size 0 and n−1, so the recurrence relation is</p>
<p>This is the same relation as for insertion sort and selection sort, and it solves to worst case T(n) = O(n²).</p>
<p>In the most balanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size n/2, so the recurrence relation is</p>
<p>The master theorem tells us that T(n) = O(n log n).</p>
<p>The outline of a formal proof of the O(n log n) expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. When the input is a random permutation, the rank of the pivot is uniform random from 0 to n − 1. Then the resulting parts of the partition have sizes i and n − i − 1, and i is uniform random from 0 to n − 1. So, averaging over all possible splits and noting that the number of comparisons for the partition is n − 1, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:</p>
<p>Solving the recurrence gives C(n) = 2n ln n ≈ 1.39n log₂ n.</p>
<p>This means that, on average, quicksort performs only about 39% worse than in its best case. In this sense it is closer to the best case than the worst case. Also note that a comparison sort cannot use less than log₂(n!) comparisons on average to sort n items (as explained in the article Comparison sort) and in case of large n, Stirling's approximation yields log₂(n!) ≈ n(log₂ n − log₂ e), so quicksort is not much worse than an ideal comparison sort. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.</p>
<p></p>
<p><img class="mwe-math-fallback-image-inline tex" alt="
\begin{align}
T(n) &amp; = \frac{1}{n} \sum_{i=1}^{n} (T(i-1)+T(n-i)) + dn \\
     &amp; = \frac{2}{n} \sum_{i=1}^{n-1} T(i) + dn
\end{align}
" src="//upload.wikimedia.org/math/5/9/f/59fa8ec0a795b3f25b79a0885ecfa604.png"></p>
<p><b>Assertion</b> A(m) :  for all m ≥ 1<br>
<b>Base case</b> A(0) : Holds for b ≥ c<br>
<b>Induction step</b> : Assuming A(m) holds for all m&lt;n, we have to prove A(n).</p>
<h2>References</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>Merge sort's most common implementation does not sort in place; therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for versions that need only n/2 extra spaces).</p>
<p>With each test that fails to find a match at the probed position, the search is continued with one or other of the two sub-intervals, each at most half the size. More precisely, if the number of items, N, is odd then both sub-intervals will contain (N−1)/2 elements, while if N is even then the two sub-intervals contain N/2−1 and N/2 elements.</p>
<p>If the original number of items is N then after the first iteration there will be at most N/2 items remaining, then at most N/4 items, at most N/8 items, and so on. In the worst case, when the value is not in the list, the algorithm must continue iterating until the span has been made empty; this will have taken at most ⌊log<sub>2</sub>(N)+1⌋ iterations, where the ⌊ ⌋ notation denotes the floor function that rounds its argument down to an integer. This worst case analysis is tight: for any N there exists a query that takes exactly ⌊log<sub>2</sub>(N)+1⌋ iterations. When compared to linear search, whose worst-case behaviour is N iterations, we see that binary search is substantially faster as N grows large. For example, to search a list of one million items takes as many as one million iterations with linear search, but never more than twenty iterations with binary search. However, a binary search can only be performed if the list is in sorted order.</p>
<p>log<sub>2</sub>(N)−1 is the expected number of probes in an average successful search, and the worst case is log<sub>2</sub>(N), just one more probe. If the list is empty, no probes at all are made. Thus binary search is a logarithmic algorithm and executes in O(log N) time. In most cases it is considerably faster than a linear search. It can be implemented using iteration, or recursion. In some languages it is more elegantly expressed recursively; however, in some C-based languages tail recursion is not eliminated and the recursive version requires more stack space.</p>
<p>Binary search can interact poorly with the memory hierarchy (i.e. caching), because of its random-access nature. For in-memory searching, if the span to be searched is small, a linear search may have superior performance simply because it exhibits better locality of reference. For external searching, care must be taken or each of the first several probes will lead to a disk seek. A common method is to abandon binary searching for linear searching as soon as the size of the remaining span falls below a small value such as 8 or 16 or even more in recent computers. The exact value depends entirely on the machine running the algorithm.</p>
<p>Notice that for multiple searches with a fixed value for N, then (with the appropriate regard for integer division), the first iteration always selects the middle element at N/2, and the second always selects either N/4 or 3N/4, and so on. Thus if the array's key values are in some sort of slow storage (on a disc file, in virtual memory, not in the cpu's on-chip memory), keeping those three keys in a local array for a special preliminary search will avoid accessing widely separated memory. Escalating to seven or fifteen such values will allow further levels at not much cost in storage. On the other hand, if the searches are frequent and not separated by much other activity, the computer's various storage control features will more or less automatically promote frequently accessed elements into faster storage.</p>
<p>When multiple binary searches are to be performed for the same key in related lists, fractional cascading can be used to speed up successive searches after the first one.</p>
<p>Even though in theory binary search is almost always faster than linear search, in practice even on small arrays (around 64 items or less) it might be infeasible to ever use binary search. On large unsorted arrays, it only makes sense to binary search if the number of searches is large enough, because the initial time to sort the array is comparable to log(n) linear searches </p>
<h2>Variations</h2>
<p>Let d be the minimum number of children an internal (non-root) node can have. For an ordinary B-tree, d=⌈m/2⌉.</p>
<p>Comer (1979, p. 127) and Cormen et al. (2001, pp. 383–384) give the worst case height of a B-tree (where the root node is considered to have height 0) as</p>
<h2>Algorithms</h2>
<p>Searching is similar to searching a binary search tree. Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search chooses the child pointer (subtree) whose separation values are on either side of the search value.</p>
<p>Binary search is typically (but not necessarily) used within nodes to find the separation values and child tree of interest.</p>
<p>All insertions start at a leaf node. To insert a new element, search the tree to find the leaf node where the new element should be added. Insert the new element into that node with the following steps:</p>
<p>If the splitting goes all the way up to the root, it creates a new root with a single separator value and two children, which is why the lower bound on the size of internal nodes does not apply to the root. The maximum number of elements per node is U−1. When a node is split, one element moves to the parent, but one element is added. So, it must be possible to divide the maximum number U−1 of elements into two legal nodes. If this number is odd, then U=2L and one of the new nodes contains (U−2)/2 = L−1 elements, and hence is a legal node, and the other contains one more element, and hence it is legal too. If U−1 is even, then U=2L−1, so there are 2L−2 elements in the node. Half of this number is L−1, which is the minimum number of elements allowed per node.</p>
<p>An improved algorithm (Mond &amp; Raz 1985) supports a single pass down the tree from the root to the node where the insertion will take place, splitting any full nodes encountered on the way. This prevents the need to recall the parent nodes into memory, which may be expensive if the nodes are on secondary storage. However, to use this improved algorithm, we must be able to send one element to the parent and split the remaining U−2 elements into two legal nodes, without adding a new element. This requires U = 2L rather than U = 2L−1, which accounts for why some textbooks impose this requirement in defining B-trees.</p>
<p>There are two popular strategies for deletion from a B-tree.</p>
<p>The algorithm below uses the former strategy.</p>
<p>Ultimately, used sensibly, any kind of hash table algorithm is usually fast enough; and the percentage of a calculation spent in hash table code is low. Memory usage is rarely considered excessive. Therefore, in most cases the differences between these algorithms are marginal, and other considerations typically come into play.</p>
<p>A hybrid of chaining and open addressing, coalesced hashing links together chains of nodes within the table itself. Like open addressing, it achieves space usage and (somewhat diminished) cache advantages over chaining. Like chaining, it does not exhibit clustering effects; in fact, the table can be efficiently filled to a high density. Unlike chaining, it cannot have more elements than table slots.</p>
<p>Another alternative open-addressing solution is cuckoo hashing, which ensures constant lookup time in the worst case, and constant amortized time for insertions and deletions. It uses two or more hash functions, which means any key/value pair could be in two or more locations. For lookup, the first hash function is used; if the key/value is not found, then the second hash function is used, and so on. If a collision happens during insertion, then the key is re-hashed with the second hash function to map it to another bucket. If all hash functions are used and there is still a collision, then the key it collided with is removed to make space for the new key, and the old key is re-hashed with one of the other hash functions, which maps it to another bucket. If that location also results in a collision, then the process repeats until there is no collision or the process traverses all the buckets, at which point the table is resized. By combining multiple hash functions with multiple cells per bucket, very high space utilisation can be achieved.</p>
<p>Another alternative open-addressing solution is hopscotch hashing, which combines the approaches of cuckoo hashing and linear probing, yet seems in general to avoid their limitations. In particular it works well even when the load factor grows beyond 0.9. The algorithm is well suited for implementing a resizable concurrent hash table.</p>
<p>The hopscotch hashing algorithm works by defining a neighborhood of buckets near the original hashed bucket, where a given entry is always found. Thus, search is limited to the number of entries in this neighborhood, which is logarithmic in the worst case, constant on average, and with proper alignment of the neighborhood typically requires one cache miss. When inserting an entry, one first attempts to add it to a bucket in the neighborhood. However, if all buckets in this neighborhood are occupied, the algorithm traverses buckets in sequence until an open slot (an unoccupied bucket) is found (as in linear probing). At that point, since the empty bucket is outside the neighborhood, items are repeatedly displaced in a sequence of hops. (This is similar to cuckoo hashing, but with the difference that in this case the empty slot is being moved into the neighborhood, instead of items being moved out with the hope of eventually finding an empty slot.) Each hop brings the open slot closer to the original neighborhood, without invalidating the neighborhood property of any of the buckets along the way. In the end, the open slot has been moved into the neighborhood, and the entry being inserted can be added to it.</p>
<p>One interesting variation on double-hashing collision resolution is Robin Hood hashing. The idea is that a new key may displace a key already inserted, if its probe count is larger than that of the key at the current position. The net effect of this is that it reduces worst case search times in the table. This is similar to ordered hash tables except that the criterion for bumping a key does not depend on a direct relationship between the keys. Since both the worst case and the variation in the number of probes is reduced dramatically, an interesting variation is to probe the table starting at the expected successful probe value and then expand from that position in both directions. External Robin Hashing is an extension of this algorithm where the table is stored in an external file and each table position corresponds to a fixed-sized page or bucket with B records.</p>
<p>2-choice hashing employs 2 different hash functions, h<sub>1</sub>(x) and h<sub>2</sub>(x), for the hash table. Both hash functions are used to compute two table locations. When an object is inserted in the table, then it is placed in the table location that contains fewer objects (with the default being the h<sub>1</sub>(x) table location if there is equality in bucket size). 2-choice hashing employs the principle of the power of two choices.</p>
<h2>Dynamic resizing</h2>
<p>The good functioning of a hash table depends on the fact that the table size is proportional to the number of entries. With a fixed size, and the common structures, it is similar to linear search, except with a better constant factor. In some cases, the number of entries may be definitely known in advance, for example keywords in a language. More commonly, this is not known for sure, if only due to later changes in code and data. It is one serious, although common, mistake to not provide any way for the table to resize. A general-purpose hash table "class" will almost always have some way to resize, and it is good practice even for simple "custom" tables. An implementation should check the load factor, and do something if it becomes too large (this needs to be done only on inserts, since that is the only thing that would increase it).</p>
<p>To keep the load factor under a certain limit, e.g. under 3/4, many table implementations expand the table when items are inserted. For example, in Java's <strong>HashMap</strong> class the default load factor threshold for table expansion is 0.75 and in Python's <strong>dict</strong>, table size is resized when load factor is greater than 2/3.</p>
<p>Since buckets are usually implemented on top of a dynamic array and any constant proportion for resizing greater than 1 will keep the load factor under the desired limit, the exact choice of the constant is determined by the same space-time tradeoff as for dynamic arrays.</p>
<p>A right rotate will move the first element to the third position, the second to the first and the third to the second. Here are two equivalent visualizations of this process:</p>
<p><i>
apple                         banana
banana    ===right rotate==&gt;  cucumber
cucumber                      apple
</i></p>
<p><i>
cucumber                      apple
banana    ===left rotate==&gt;   cucumber
apple                         banana
</i></p>
<p>A stack is usually represented in computers by a block of memory cells, with the "bottom" at a fixed location, and the stack pointer holding the address of the current "top" cell in the stack. The top and bottom terminology are used irrespective of whether the stack actually grows towards lower memory addresses or towards higher memory addresses.</p>
<p>Pushing an item on to the stack adjusts the stack pointer by the size of the item (either decrementing or incrementing, depending on the direction in which the stack grows in memory), pointing it to the next cell, and copies the new top item to the stack area. Depending again on the exact implementation, at the end of a push operation, the stack pointer may point to the next unused location in the stack, or it may point to the topmost item in the stack. If the stack points to the current topmost item, the stack pointer will be updated before a new item is pushed onto the stack; if it points to the next available location in the stack, it will be updated after the new item is pushed onto the stack.</p>
<p>The code for in-order traversal in Python is given below. It will call <b>callback</b> for every node in the tree.</p>
<p><i>
def traverse_binary_tree(node, callback):
    if node is None:
        return
    traverse_binary_tree(node.leftChild, callback)
    callback(node.value)
    traverse_binary_tree(node.rightChild, callback)
</i></p>
<p>With respect to the example defined in the lead section of this article,</p>
<p>Traversal requires O(n) time, since it must visit every node. This algorithm is also O(n), so it is asymptotically optimal.</p>
<p>A binary search tree can be used to implement a simple but efficient sorting algorithm. Similar to heapsort, we insert all the values we wish to sort into a new ordered data structure—in this case a binary search tree—and then traverse it in order, building our result:</p>
<p><i>
def build_binary_tree(values):
    tree = None
    for v in values:
        tree = binary_tree_insert(tree, v)
    return tree
 
def get_inorder_traversal(root):
    '''
    Returns a list containing all the values in the tree, starting at *root*.
    Traverses the tree in-order(leftChild, root, rightChild).
    '''
    result = 
    traverse_binary_tree(root, lambda element: result.append(element))
    return result
</i></p>
<p>The worst-case time of <strong>build_binary_tree</strong> is —if you feed it a sorted list of values, it chains them into a linked list with no left subtrees. For example, <strong>build_binary_tree()</strong> yields the tree <strong>(1 (2 (3 (4 (5)))))</strong>.</p>
<p>There are several schemes for overcoming this flaw with simple binary trees; the most common is the self-balancing binary search tree. If this same procedure is done using such a tree, the overall worst-case time is O(nlog n), which is asymptotically optimal for a comparison sort. In practice, the poor cache performance and added overhead in time and space for a tree-based sort (particularly for node allocation) make it inferior to other asymptotically optimal sorts such as heapsort for static list sorting. On the other hand, it is one of the most efficient methods of incremental sorting, adding items to a list over time while keeping the list sorted at all times.</p>
<p><img class="mwe-math-fallback-image-inline tex" alt="
\begin{align}
T(n) &amp; \le  \frac{2}{n} \sum_{i=1}^{n-1} \left(a i \log i + b \right) + dn \\
     &amp; \le  \frac{2}{n} \left(\sum_{i=1}^{n-1} a i \log i \right) + 2b + dn \\
     &amp; =    \frac{2}{n} \left(\sum_{i=1}^{n/2} a i \log i + \sum_{i=n/2+1}^{n-1}(a i \log i) \right) + 2b + dn \\
     &amp; \le  \frac{2}{n} \left(\sum_{i=1}^{n/2} a i \log (n/2) + \sum_{i=n/2+1}^{n-1} a i \log n \right)+ 2b + dn \\
     &amp; =    \frac{2}{n} \left(\sum_{i=1}^{n-1} a i \log n - \sum_{i=1}^{n/2} ai \right)+ 2b + dn \\
     &amp; =    \frac{2}{n} \left(\frac{n(n-1)}{2} (a \log n) - \frac{n/2(n/2 + 1)}{2}(a) \right) + 2b +dn \\
     &amp; \le  a(n-1)\log n - \frac{n}{4}(a) + 2b + dn \\
     &amp; =    a n \log n + b - \frac{n}{4}(a) + b + dn \\
     &amp; \le  a n \log n + b \ \textrm{ for } \ a &gt; 4(b+d)
\end{align}
" src="//upload.wikimedia.org/math/7/2/b/72be45fc60b1b4e8df48923bcaa86dd6.png"></p>
<p>Using the same analysis, one can show that randomized quicksort has the desirable property that, for any input, it requires only O(n log n) expected time (averaged over all choices of pivots). However, there also exists a combinatorial proof.</p>
<p>To each execution of quicksort corresponds the following binary search tree (BST): the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted  form a random permutation.</p>
<p>Consider a BST created by insertion of a sequence  of values forming a random permutation. Let C denote the cost of creation of the BST. We have , where  is an binary random variable expressing whether during the insertion of  there was a comparison to .</p>
<p>By linearity of expectation, the expected value  of C is .</p>
<p>Fix i and j&lt;i. The values , once sorted, define j+1 intervals. The core structural observation is that  is compared to  in the algorithm if and only if  falls inside one of the two intervals adjacent to .</p>
<p>Observe that since  is a random permutation,  is also a random permutation, so the probability that  is adjacent to  is exactly .</p>
<p>We end with a short calculation: </p>
<p>The space used by quicksort depends on the version used.</p>
<p>The in-place version of quicksort has a space complexity of O(log n), even in the worst case, when it is carefully implemented using the following strategies:</p>
<p>Quicksort with in-place and unstable partitioning uses only constant additional space before making any recursive call. Quicksort must store a constant amount of information for each nested recursive call. Since the best case makes at most O(log n) nested recursive calls, it uses O(log n) space. However, without Sedgewick's trick to limit the recursive calls, in the worst case quicksort could make O(n) nested recursive calls and need O(n) auxiliary space.</p>
<p>Merge sort also has some demerits. One is its use of 2n locations; the additional n locations are commonly used because merging two sorted sets in place is more complicated and would need more comparisons and move operations. But despite the use of this space the algorithm still does a lot of work: The contents of m are first copied into left and right and later into the list result on each invocation of merge_sort (variable names according to the pseudocode above).</p>
<h2>Variants</h2>
<p>Variants of merge sort are primarily concerned with reducing the space complexity and the cost of copying.</p>
<p>A simple alternative for reducing the space overhead to n/2 is to maintain left and right as a combined structure, copy only the left part of m into temporary space, and to direct the merge routine to place the merged output into m. With this version it is better to allocate the temporary space outside the merge routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the return result statement (function merge in the pseudo code above) become superfluous.</p>
<p>In-place sorting is possible, and still stable, but is more complicated, and slightly slower, requiring non-linearithmic quasilinear time O(n log2 n) One way to sort in-place is to merge the blocks recursively. Like the standard merge sort, in-place merge sort is also a stable sort. Stable sorting of linked lists is simpler. In this case the algorithm does not use more space than that already used by the list representation, but the O(log(k)) used for the recursion trace.</p>
<p>An alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in m are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.</p>
<h2>Use with tape drives</h2>
<p>An external merge sort is practical to run using disk or tape drives when the data to be sorted is too large to fit into memory. External sorting explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just 2 record buffers and a few program variables.</p>
<p>Naming the four tape drives as A, B, C, D, with the original data on A, and using only 2 record buffers, the algorithm is similar to Bottom-up implementation, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:</p>
<p>Instead of starting with very short runs, usually a hybrid algorithm is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save 9 passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory.</p>
<p>A more sophisticated merge sort that optimizes tape (and disk) drive usage is the polyphase merge sort.</p>
<p>There are many, and they are easily confused.</p>
<p>The most significant differences are between the "exclusive" and "inclusive" forms of the bounds. In the "exclusive" bound form the span to be searched is (L+1) to (R−1), and this may seem clumsy when the span to be searched could be described in the "inclusive" form, as L to R. Although the details differ the two forms are equivalent as can be seen by transforming one version into the other. The inclusive bound form can be attained by replacing all appearances of "L" by "(L−1)" and "R" by "(R+1)" then rearranging. Thus, the initialisation of L := 0 becomes (L−1) := 0 or L := 1, and R := N+1 becomes (R+1) := N+1 or R := N. So far so good, but note now that the changes to L and R are no longer simply transferring the value of p to L or R as appropriate but now must be (R+1) := p or R := p−1, and (L−1) := p or L := p+1.</p>
<p>Thus, the gain of a simpler initialisation, done once, is lost by a more complex calculation, and which is done for every iteration. If that is not enough, the test for an empty span is more complex also, as compared to the simplicity of checking that the value of p is zero. Nevertheless, the inclusive bound form is found in many publications, such as Donald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition.</p>
<p>Another common variation uses inclusive bounds for the left bound, but exclusive bounds for the right bound. This is derived from the fact that the bounds in a language with zero-based arrays can be simply initialized to 0 and the size of the array, respectively. This mirrors the way array slices are represented in some programming languages.</p>
<p>There are two special cases to consider when deleting an element:</p>
<p>The procedures for these cases are in order below.</p>
<p>Each element in an internal node acts as a separation value for two subtrees, therefore we need to find a replacement for separation. Note that the largest element in the left subtree is still less than the separator. Likewise, the smallest element in the right subtree is still greater than the separator. Both of those elements are in leaf nodes, and either one can be the new separator for the two subtrees. Algorithmically described below:</p>
<p>Rebalancing starts from a leaf and proceeds toward the root until the tree is balanced. If deleting an element from a node has brought it under the minimum size, then some elements must be redistributed to bring all nodes up to the minimum. Usually, the redistribution involves moving an element from a sibling node that has more than the minimum number of nodes. That redistribution operation is called a <b>rotation</b>. If no sibling can spare a node, then the deficient node must be <b>merged</b> with a sibling. The merge causes the parent to lose a separator element, so the parent may become deficient and need rebalancing. The merging and rebalancing may continue all the way to the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a problem. The algorithm to rebalance the tree is as follows:</p>
<p>While freshly loaded databases tend to have good sequential behavior, this behavior becomes increasingly difficult to maintain as a database grows, resulting in more random I/O and performance challenges.</p>
<p>In applications, it is frequently useful to build a B-tree to represent a large existing collection of data and then update it incrementally using standard B-tree operations. In this case, the most efficient way to construct the initial B-tree is not to insert every element in the initial collection successively, but instead to construct the initial set of leaf nodes directly from the input, then build the internal nodes from these. This approach to B-tree construction is called bulkloading. Initially, every leaf but the last one has one extra element, which will be used to build the internal nodes.</p>
<p>For example, if the leaf nodes have maximum size 4 and the initial collection is the integers 1 through 24, we would initially construct 4 leaf nodes containing 5 values each and 1 which contains 4 values:</p>
<p>We build the next level up from the leaves by taking the last element from each leaf node except the last one. Again, each node except the last will contain one extra value. In the example, suppose the internal nodes contain at most 2 values (3 child pointers). Then the next level up of internal nodes would be:</p>
<p>This process is continued until we reach a level with only one node and it is not overfilled. In the example only the root level remains:</p>
<p>Resizing is accompanied by a full or incremental table <b>rehash</b> whereby existing items are mapped to new bucket locations.</p>
<p>To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table—followed by a rehash—when items are deleted. From the point of space-time tradeoffs, this operation is similar to the deallocation in dynamic arrays.</p>
<p>A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold r<sub>max</sub>. Then a new larger table is allocated, all the entries of the old table are removed and inserted into this new table, and the old table is returned to the free storage pool. Symmetrically, when the load factor falls below a second threshold r<sub>min</sub>, all entries are moved to a new smaller table.</p>
<p>If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries n and of the number m of operations performed.</p>
<p>For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If m elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most m − 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation.</p>
<p>Some hash table implementations, notably in real-time systems, cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually:</p>
<p>To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it is necessary to increase the size of the table by a factor of at least (r + 1)/r during resizing.</p>
<p>If it is known that key values will always increase (or decrease) monotonically, then a variation of consistent hashing can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function—and indeed hash table—both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(lg(N)) ranges to check, and binary search time for the redirection would be O(lg(lg(N))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown.</p>
<p>Linear hashing is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible look-up functions.</p>
<p>Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. This approach, called consistent hashing, is prevalent in disk-based and distributed hashes, where rehashing is prohibitively costly.</p>
<h2>Performance analysis</h2>
<p>Popping the stack is simply the inverse of pushing. The topmost item in the stack is removed and the stack pointer is updated, in the opposite order of that used in the push operation.</p>
<p>Most CPUs have registers that can be used as stack pointers. Processor families like the x86, Z80, 6502, and many others have special instructions that implicitly use a dedicated (hardware) stack pointer to conserve opcode space. Some processors, like the PDP-11 and the 68000, also have special addressing modes for implementation of stacks, typically with a semi-dedicated stack pointer as well (such as A7 in the 68000). However, in most processors, several different registers may be used as additional stack pointers as needed (whether updated via addressing modes or via add/sub instructions).</p>
<p>The x87 floating point architecture is an example of a set of registers organised as a stack where direct access to individual registers (relative the current top) is also possible. As with stack-based machines in general, having the top-of-stack as an implicit argument allows for a small machine code footprint with a good usage of bus bandwidth and code caches, but it also prevents some types of optimizations possible on processors permitting random access to the register file for all (two or three) operands. A stack structure also makes superscalar implementations with register renaming (for speculative execution) somewhat more complex to implement, although it is still feasible, as exemplified by modern x87 implementations.</p>
<p>Sun SPARC, AMD Am29000, and Intel i960 are all examples of architectures using register windows within a register-stack as another strategy to avoid the use of slow main memory for function arguments and return values.</p>
<p>There are also a number of small microprocessors that implements a stack directly in hardware and some microcontrollers have a fixed-depth stack that is not directly accessible. Examples are the PIC microcontrollers, the Computer Cowboys MuP21, the Harris RTX line, and the Novix NC4016. Many stack-based microprocessors were used to implement the programming language Forth at the microcode level. Stacks were also used as a basis of a number of mainframes and mini computers. Such machines were called stack machines, the most famous being the Burroughs B5000.</p>
<h2>Applications</h2>
<p>Stacks are present everyday life, from the books in a library, to the blank sheets of paper in a printer tray. All these applications follow the Last In First Out (LIFO) logic, which means that (for example) a book is added on top of a pile of books, while removing a book from a pile also takes the book on top of a pile.</p>
<p>Below are a few applications of stacks in computing.</p>
<p>Calculators employing reverse Polish notation use a stack structure to hold values. Expressions can be represented in prefix, postfix or infix notations and conversion from one form to another may be accomplished using a stack. Many compilers use a stack for parsing the syntax of expressions, program blocks etc. before translating into low level code. Most programming languages are context-free languages, allowing them to be parsed with stack based machines.</p>
<p>Another important application of stacks is backtracking. Consider a simple example of finding the correct path in a maze. There are a series of points, from the starting point to the destination. We start from one point. To reach the final destination, there are several paths. Suppose we choose a random path. After following a certain path, we realise that the path we have chosen is wrong. So we need to find a way by which we can return to the beginning of that path. This can be done with the use of stacks. With the help of stacks, we remember the point where we have reached. This is done by pushing that point into the stack. In case we end up on the wrong path, we can pop the last point from the stack and thus return to the last point and continue our quest to find the right path. This is called backtracking.</p>
<p>A number of programming languages are stack-oriented, meaning they define most basic operations (adding two numbers, printing a character) as taking their arguments from the stack, and placing any return values back on the stack. For example, PostScript has a return stack and an operand stack, and also has a graphics state stack and a dictionary stack. Many virtual machines are also stack-oriented, including the p-code machine and the Java Virtual Machine.</p>
<h2>Types</h2>
<p>There are many types of binary search trees. AVL trees and red-black trees are both forms of self-balancing binary search trees. A splay tree is a binary search tree that automatically moves frequently accessed elements nearer to the root. In a treap (tree heap), each node also holds a (randomly chosen) priority and the parent node has higher priority than its children. Tango trees are trees optimized for fast searches.</p>
<p>Two other titles describing binary search trees are that of a complete and degenerate tree.</p>
<p>A complete binary tree is a binary tree, which is completely filled, with the possible exception of the bottom level, which is filled from left to right. In complete binary tree, all nodes are far left as possible. It is a tree with n levels, where for each level d &lt;= n - 1, the number of existing nodes at level d is equal to 2d. This means all possible nodes exist at these levels. An additional requirement for a complete binary tree is that for the nth level, while every node does not have to exist, the nodes that do exist must fill from left to right.</p>
<p>From a bit complexity viewpoint, variables such as left and right do not use constant space; it takes O(log n) bits to index into a list of n items. Because there are such variables in every stack frame, quicksort using Sedgewick's trick requires O((log n)²) bits of space. This space requirement isn't too terrible, though, since if the list contained distinct elements, it would need at least O(n log n) bits of space.</p>
<p>Another, less common, not-in-place, version of quicksort uses O(n) space for working storage and can implement a stable sort. The working storage allows the input array to be easily partitioned in a stable manner and then copied back to the input array for successive recursive calls. Sedgewick's optimization is still appropriate.</p>
<h2>Relation to other algorithms</h2>
<p>Quicksort is a space-optimized version of the binary tree sort. Instead of inserting items sequentially into an explicit tree, quicksort organizes them concurrently into a tree that is implied by the recursive calls. The algorithms make exactly the same comparisons, but in a different order. An often desirable property of a sorting algorithm is stability - that is the order of elements that compare equal is not changed, allowing controlling order of multikey tables (e.g. directory or folder listings) in a natural way. This property is hard to maintain for in situ (or in place) quicksort (that uses only constant additional space for pointers and buffers, and logN additional space for the management of explicit or implicit recursion). For variant quicksorts involving extra memory due to representations using pointers (e.g. lists or trees) or files (effectively lists), it is trivial to maintain stability. The more complex, or disk-bound, data structures tend to increase time cost, in general making increasing use of virtual memory or disk.</p>
<p>The most direct competitor of quicksort is heapsort. Heapsort's worst-case running time is always O(n log n). But, heapsort is assumed to be on average somewhat slower than standard in-place quicksort. This is still debated and in research, with some publications indicating the opposite. Introsort is a variant of quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case running time.</p>
<p>Quicksort also competes with mergesort, another recursive sort algorithm but with the benefit of worst-case O(n log n) running time. Mergesort is a stable sort, unlike standard in-place quicksort and heapsort, and can be easily adapted to operate on linked lists and very large lists stored on slow-to-access media such as disk storage or network attached storage. Although quicksort can easily be implemented as a stable sort using linked lists, it will often suffer from poor pivot choices without random access. The main disadvantage of mergesort is that, when operating on arrays, efficient implementations require O(n) auxiliary space, whereas the variant of quicksort with in-place partitioning and tail recursion uses only O(log n) space. (Note that when operating on linked lists, mergesort only requires a small, constant amount of auxiliary storage.)</p>
<p>Bucket sort with two buckets is very similar to quicksort; the pivot in this case is effectively the value in the middle of the value range, which does well on average for uniformly distributed inputs.</p>
<p>A selection algorithm chooses the kth smallest of a list of numbers; this is an easier problem in general than sorting. One simple but effective selection algorithm works nearly in the same manner as quicksort, and is accordingly known as quickselect. The difference is that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist which contains the desired element. This change lowers the average complexity to linear or O(n) time, which is optimal for selection, but worst-case time is still O(n2).</p>
<h2>Optimizing merge sort</h2>
<p>On modern computers, locality of reference can be of paramount importance in software optimization, because multilevel memory hierarchies are used. Cache-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the <b>tiled merge sort</b> algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as insertion sort, to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance on machines that benefit from cache optimization. (LaMarca &amp; Ladner 1997)</p>
<p>Kronrod (1969) suggested an alternative version of merge sort that uses constant additional space. This algorithm was later refined. (Katajainen, Pasanen &amp; Teuhola 1996)</p>
<p>Also, many applications of external sorting use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory.</p>
<h2>Parallel processing</h2>
<p>Merge sort parallelizes well due to use of the divide-and-conquer method. A parallel implementation is shown in pseudo-code in the third edition of Cormen, Leiserson, Rivest, and Stein's Introduction to Algorithms. This algorithm uses a parallel merge algorithm to not only parallelize the recursive division of the array, but also the merge operation. It performs well in practice when combined with a fast stable sequential sort, such as insertion sort, and a fast sequential merge as a base case for merging small arrays. Merge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure O(1) merge. Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW PRAM with n processors by performing partitioning implicitly. Powers further shows that a pipelined version of Batcher's Bitonic Mergesort at O(log2n) time on a butterfly sorting network is in practice actually faster than his O(log n) sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.</p>
<h2>Comparison with other sort algorithms</h2>
<p>Although heapsort has the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ(n). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.</p>
<p>As of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to insertion sort when fewer than seven array elements are being sorted. Python uses Timsort, another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in Java SE 7, on the Android platform, and in GNU Octave.</p>
<h2>Notes</h2>
<p>A different variation involves abandoning the L and R pointers and using a current position p and a width w. At each iteration, the position p is adjusted and the width w is halved. Knuth states, "It is possible to do this, but only if extreme care is paid to the details."</p>
<p>There is no particular requirement that the array being searched has the bounds 1 to N. It is possible to search a specified range, elements first to last instead of 1 to N. All that is necessary is that the initialization of the bounds be L := first−1 and R := last+1, then all proceeds as before.</p>
<p>The elements of the list are not necessarily all unique. If one searches for a value that occurs multiple times in the list, the index returned will be of the first-encountered equal element, and this will not necessarily be that of the first, last, or middle element of the run of equal-key elements but will depend on the positions of the values. Modifying the list even in seemingly unrelated ways such as adding elements elsewhere in the list may change the result.</p>
<p>If the location of the first and/or last equal element needs to be determined, this can be done efficiently with a variant of the binary search algorithms which perform only one inequality test per iteration. See deferred detection of equality.</p>
<p>Several algorithms closely related to or extending binary search exist. For instance, <b>noisy binary search</b> solves the same class of projects as regular binary search, with the added complexity that any given test can return a false value at random. (Usually, the number of such erroneous results are bounded in some way, either in the form of an average error rate, or in the total number of errors allowed per element in the search space.) Optimal algorithms for several classes of noisy binary search problems have been known since the late seventies, and more recently, optimal algorithms for noisy binary search in quantum computers (where several elements can be tested at the same time) have been discovered.</p>
<p>An <b>exponential search</b> (also called a <b>one-sided search</b>) searches from a starting point within the array and either expects that the element  being sought is nearby or the upper (lower) bound on the array is unknown. Starting with a step size of 1 and doubling with each step, the method looks for a number &gt;= (&lt;=) . Once the upper (lower) bound is found, then the method proceeds with a binary search. The complexity of the search is  if the sought element is in the nth array position. This depends only on  and not on the size of the array.</p>
<p>An interpolated search tries to guess the location of the element  you're searching for, typically by calculating a midpoint based on the lowest and highest value and assuming a fairly even distribution of values. When  has been determined an exponential search is performed.</p>
<h2>Implementation issues</h2>
<p>Although the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky… — Donald Knuth</p>
<p>When Jon Bentley assigned it as a problem in a course for professional programmers, he found that an astounding ninety percent failed to code a binary search correctly after several hours of working on it, and another study shows that accurate code for it is only found in five out of twenty textbooks. Furthermore, Bentley's own implementation of binary search, published in his 1986 book Programming Pearls, contains an error that remained undetected for over twenty years.</p>
<h2>In filesystems</h2>
<p>In addition to its use in databases, the B-tree is also used in filesystems to allow quick random access to an arbitrary block in a particular file. The basic problem is turning the file block  address into a disk block (or perhaps to a cylinder-head-sector) address.</p>
<p>Some operating systems require the user to allocate the maximum size of the file when the file is created. The file can then be allocated as contiguous disk blocks. Converting to a disk block: the operating system just adds the file block address to the starting disk block of the file. The scheme is simple, but the file cannot exceed its created size.</p>
<p>Other operating systems allow a file to grow. The resulting disk blocks may not be contiguous, so mapping logical blocks to physical blocks is more involved.</p>
<p>MS-DOS, for example, used a simple File Allocation Table (FAT). The FAT has an entry for each disk block, and that entry identifies whether its block is used by a file and if so, which block (if any) is the next disk block of the same file. So, the allocation of each file is represented as a linked list in the table. In order to find the disk address of file block , the operating system (or disk utility) must sequentially follow the file's linked list in the FAT. Worse, to find a free disk block, it must sequentially scan the FAT. For MS-DOS, that was not a huge penalty because the disks and files were small and the FAT had few entries and relatively short file chains. In the FAT12 filesystem (used on floppy disks and early hard disks), there were no more than 4,080  entries, and the FAT would usually be resident in memory. As disks got bigger, the FAT architecture began to confront penalties. On a large disk using FAT, it may be necessary to perform disk reads to learn the disk location of a file block to be read or written.</p>
<p>In the simplest model, the hash function is completely unspecified and the table does not resize. For the best possible choice of hash function, a table of size k with open addressing has no collisions and holds up to k elements, with a single comparison for successful lookup, and a table of size k with chaining and n keys has the minimum max(0, n-k) collisions and O(1 + n/k) comparisons for lookup. For the worst choice of hash function, every insertion causes a collision, and hash tables degenerate to linear search, with Ω(n) amortized comparisons per insertion and up to n comparisons for a successful lookup.</p>
<p>Adding rehashing to this model is straightforward. As in a dynamic array, geometric resizing by a factor of b implies that only n/bi keys are inserted i or more times, so that the total number of insertions is bounded above by bn/(b-1), which is O(n). By using rehashing to maintain n &lt; k, tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function.</p>
<p>In more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform, the assumption is called "simple uniform hashing" and it can be shown that hashing with chaining requires Θ(1 + n/k) comparisons on average for an unsuccessful lookup, and hashing with open addressing requires Θ(1/(1 - n/k)). Both these bounds are constant, if we maintain n/k &lt; c using table resizing, where c is a fixed constant less than 1.</p>
<h2>Features</h2>
<p>The main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance, so that the bucket array can be allocated once with the optimum size and never resized.</p>
<p>If the set of key-value pairs is fixed and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, bucket table size, and internal data structures. In particular, one may be able to devise a hash function that is collision-free, or even perfect (see below). In this case the keys need not be stored in the table.</p>
<p>Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small. (However, in some cases the high cost of computing the hash function can be mitigated by saving the hash value together with the key.)</p>
<p>For certain string processing applications, such as spell-checking, hash tables may be less efficient than tries, finite automata, or Judy arrays. Also, if each key is represented by a small enough number of bits, then, instead of a hash table, one may use the key directly as the index into an array of values. Note that there are no collisions in this case.</p>
<p>Almost all calling conventions – computer runtime memory environments – use a special stack (the "call stack") to hold information about procedure/function calling and nesting in order to switch to the context of the called function and restore to the caller function when the calling finishes. The functions follow a runtime protocol between caller and callee to save arguments and return value on the stack. Stacks are an important way of supporting nested or recursive function calls. This type of stack is used implicitly by the compiler to support CALL and RETURN statements (or their equivalents) and is not manipulated directly by the programmer.</p>
<p>Some programming languages use the stack to store data that is local to a procedure. Space for local data items is allocated from the stack when the procedure is entered, and is deallocated when the procedure exits. The C programming language is typically implemented in this way. Using the same stack for both data and procedure calls has important security implications (see below) of which a programmer must be aware in order to avoid introducing serious security bugs into a program.</p>
<h2>Security</h2>
<p>Some computing environments use stacks in ways that may make them vulnerable to security breaches and attacks. Programmers working in such environments must take special care to avoid the pitfalls of these implementations.</p>
<p>For example, some programming languages use a common stack to store both data local to a called procedure and the linking information that allows the procedure to return to its caller. This means that the program moves data into and out of the same stack that contains critical return addresses for the procedure calls. If data is moved to the wrong location on the stack, or an oversized data item is moved to a stack location that is not large enough to contain it, return information for procedure calls may be corrupted, causing the program to fail.</p>
<p>Malicious parties may attempt a stack smashing attack that takes advantage of this type of implementation by providing oversized data input to a program that does not check the length of input. Such a program may copy the data in its entirety to a location on the stack, and in so doing it may change the return addresses for procedures that have called it. An attacker can experiment to find a specific type of data that can be provided to such a program such that the return address of the current procedure is reset to point to an area within the stack itself (and within the data provided by the attacker), which in turn contains instructions that carry out unauthorized operations.</p>
<p>This type of attack is a variation on the buffer overflow attack and is an extremely frequent source of security breaches in software, mainly because some of the most popular compilers use a shared stack for both data and procedure calls, and do not verify the length of data items. Frequently programmers do not write code to verify the size of data items, either, and when an oversized or undersized data item is copied to the stack, a security breach may occur.</p>
<h2>See also</h2>
<h2>References</h2>
<h2>Further reading</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>A degenerate tree is a tree where for each parent node, there is only one associated child node. It is unbalanced and, in the worst case, performance degrades to that of a linked list. If your added node function does not handle re-balancing, then you can easily construct a degenerate tree by feeding it with data that is already sorted.What this means is that in a performance measurement, the tree will essentially behave like a linked list data structure.</p>
<p>D. A. Heger (2004) presented a performance comparison of binary search trees. Treap was found to have the best average performance, while red-black tree was found to have the smallest amount of performance variations.</p>
<p>If we do not plan on modifying a search tree, and we know exactly how often each item will be accessed, we can construct an optimal binary search tree, which is a search tree where the average cost of looking up an item (the expected search cost) is minimized.</p>
<p>Even if we only have estimates of the search costs, such a system can considerably speed up lookups on average. For example, if you have a BST of English words used in a spell checker, you might balance the tree based on word frequency in text corpora, placing words like the near the root and words like agerasia near the leaves. Such a tree might be compared with Huffman trees, which similarly seek to place frequently used items near the root in order to produce a dense information encoding; however, Huffman trees store data elements only in leaves, and these elements need not be ordered.</p>
<p>If we do not know the sequence in which the elements in the tree will be accessed in advance, we can use splay trees which are asymptotically as good as any static search tree we can construct for any particular sequence of lookup operations.</p>
<p>Alphabetic trees are Huffman trees with the additional constraint on order, or, equivalently, search trees with the modification that all elements are stored in the leaves. Faster algorithms exist for optimal alphabetic binary trees (OABTs).</p>
<h2>See also</h2>
<h2>References</h2>
<h2>Further reading</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>A variant of quickselect, the median of medians algorithm, chooses pivots more carefully, ensuring that the pivots are near the middle of the data (between the 30th and 70th percentiles), and thus has guaranteed linear time – worst-case O(n). This same pivot strategy can be used to construct a variant of quickselect (median of medians quicksort) with worst-case O(n) time. However, the overhead of choosing the pivot is significant, so this is generally not used in practice.</p>
<p>More abstractly, given a worst-case O(n) selection algorithm, one can use it to find the ideal pivot (the median) at every step of quicksort, producing a variant with worst-case O(n log n) running time. In practical implementations this variant is considerably slower on average, but it is of theoretical interest, showing how an optimal selection algorithm can yield an optimal sorting algorithm.</p>
<p>Richard Cole and David C. Kandathil, in 2004, discovered a one-parameter family of sorting algorithms, called partition sorts, which on average (with all input orderings equally likely) perform at most  comparisons (close to the information theoretic lower bound) and  operations; at worst they perform  comparisons (and also operations); these are in-place, requiring only additional  space. Practical efficiency and smaller variance in performance were demonstrated against optimised quicksorts (of Sedgewick and Bentley-McIlroy).</p>
<h2>See also</h2>
<h2>Notes</h2>
<h2>References</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<h2>References</h2>
<p>In a practical implementation, the variables used to represent the indices will often be of finite size, hence only capable of representing a finite range of values. For example, 32-bit unsigned integers can only hold values from 0 to 4294967295. 32-bit signed integers can only hold values from -2147483648 to 2147483647. If the binary search algorithm is to operate on large arrays, this has three implications:</p>
<h2>Language support</h2>
<p>Many standard libraries provide a way to do a binary search:</p>
<h2>See also</h2>
<h2>References</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>TOPS-20 (and possibly TENEX) used a 0 to 2 level tree that has similarities to a B-tree. A disk block was 512 36-bit words. If the file fit in a 512 (29) word block, then the file directory would point to that physical disk block. If the file fit in 218 words, then the directory would point to an aux index; the 512 words of that index would either be NULL (the block isn't allocated) or point to the physical address of the block. If the file fit in 227 words, then the directory would point to a block holding an aux-aux index; each entry would either be NULL or point to an aux index. Consequently, the physical disk block for a 227 word file could be located in two disk reads and read on the third.</p>
<p>Apple's filesystem HFS+, Microsoft's NTFS, AIX (jfs2) and some Linux filesystems, such as btrfs and Ext4, use B-trees.</p>
<p>B*-trees are used in the HFS and Reiser4 file systems.</p>
<h2>Variations</h2>
<p>Lehman and Yao showed that all read locks could be avoided (and thus concurrent access greatly improved) by linking the tree blocks at each level together with a "next" pointer. This results in a tree structure where both insertion and search operations descend from the root to the leaf. Write locks are only required as a tree block is modified. This maximizes access concurrency by multiple users, an important consideration for databases and/or other B-tree based ISAM storage methods. The cost associated with this improvement is that empty pages cannot be removed from the btree during normal operations. (However, see  for various strategies to implement node merging, and source code at.)</p>
<p>United States Patent 5283894, granted in 1994, appears to show a way to use a 'Meta Access Method'  to allow concurrent B+ tree access and modification without locks. The technique accesses the tree 'upwards' for both searches and updates by means of additional in-memory indexes that point at the blocks in each level in the block cache. No reorganization for deletes is needed and there are no 'next' pointers in each block as in Lehman and Yao.</p>
<h2>See also</h2>
<h2>Notes</h2>
<h2>References</h2>
<p>The entries stored in a hash table can be enumerated efficiently (at constant cost per entry), but only in some pseudo-random order. Therefore, there is no efficient way to locate an entry whose key is nearest to a given key. Listing all n entries in some specific order generally requires a separate sorting step, whose cost is proportional to log(n) per entry. In comparison, ordered search trees have lookup and insertion cost proportional to log(n), but allow finding the nearest key at about the same cost, and ordered enumeration of all entries at constant cost per entry.</p>
<p>If the keys are not stored (because the hash function is collision-free), there may be no easy way to enumerate the keys that are present in the table at any given moment.</p>
<p>Although the average cost per operation is constant and fairly small, the cost of a single operation may be quite high. In particular, if the hash table uses dynamic resizing, an insertion or deletion operation may occasionally take time proportional to the number of entries. This may be a serious drawback in real-time or interactive applications.</p>
<p>Hash tables in general exhibit poor locality of reference—that is, the data to be accessed is distributed seemingly at random in memory. Because hash tables cause access patterns that jump around, this can trigger microprocessor cache misses that cause long delays. Compact data structures such as arrays searched with linear search may be faster, if the table is relatively small and keys are compact. The optimal performance point varies from system to system.</p>
<p>Hash tables become quite inefficient when there are many collisions. While extremely uneven hash distributions are extremely unlikely to arise by chance, a malicious adversary with knowledge of the hash function may be able to supply information to a hash that creates worst-case behavior by causing excessive collisions, resulting in very poor performance, e.g. a denial of service attack. In critical applications, universal hashing can be used; a data structure with better worst-case guarantees may be preferable.</p>
<h2>Uses</h2>
<p>Hash tables are commonly used to implement many types of in-memory tables. They are used to implement associative arrays (arrays whose indices are arbitrary strings or other complicated objects), especially in interpreted programming languages like Ruby, Python, and PHP.</p>
<p>When storing a new item into a multimap and a hash collision occurs, the multimap unconditionally stores both items.</p>
<p>When storing a new item into a typical associative array and a hash collision occurs, but the actual keys themselves are different, the associative array likewise stores both items. However, if the key of the new item exactly matches the key of an old item, the associative array typically erases the old item and overwrites it with the new item, so every item in the table has a unique key.</p>
<p>Hash tables may also be used as disk-based data structures and database indices (such as in dbm) although B-trees are more popular in these applications.</p>
<h2>External links</h2>
<h2>Navigation menu</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
<p>Hash tables can be used to implement caches, auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries—usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value.</p>
<p>Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not.</p>
<p>Those structures can therefore be used to implement a set data structure, which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets.</p>
<p>Several dynamic languages, such as Perl, Python, JavaScript, and Ruby, use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method.</p>
<p>Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single string pool implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing, and can be used with many other kinds of data (expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.)</p>
<h2>Implementations</h2>
<p>Many programming languages provide hash table functionality, either as built-in associative arrays or as standard library modules. In C++11, for example, the <strong>unordered_map</strong> class provides hash tables for keys and values of arbitrary type.</p>
<p>In PHP 5, the Zend 2 engine uses one of the hash functions from Daniel J. Bernstein to generate the hash values used in managing the mappings of data pointers stored in a hash table. In the PHP source code, it is labelled as <strong>DJBX33A</strong> (Daniel J. Bernstein, Times 33 with Addition).</p>
<p>Python's built-in hash table implementation, in the form of the <strong>dict</strong> type, as well as Perl's hash type (%) are used internally to implement namespaces and therefore need to pay more attention to security, i.e. collision attacks.</p>
<p>In the .NET Framework, support for hash tables is provided via the non-generic <strong>Hashtable</strong> and generic <strong>Dictionary</strong> classes, which store key-value pairs, and the generic <strong>HashSet</strong> class, which stores only values.</p>
<h2>History</h2>
<p>The idea of hashing arose independently in different places. In January 1953, H. P. Luhn wrote an internal IBM memorandum that used hashing with chaining. G. N. Amdahl, E. M. Boehme, N. Rochester, and Arthur Samuel implemented a program using hashing at about the same time. Open addressing with linear probing (relatively prime stepping) is credited to Amdahl, but Ershov (in Russia) had the same idea.</p>
<h2>See also</h2>
<p>There are several data structures that use hash functions but cannot be considered special cases of hash tables:</p>
<h2>References</h2>
<h2>Further reading</h2>
<h2>External links</h2>
<h2>Navigation menu</h2>
